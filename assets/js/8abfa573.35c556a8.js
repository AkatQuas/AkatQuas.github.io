"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[7967],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>m});var a=n(67294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var i=a.createContext({}),c=function(e){var t=a.useContext(i),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},p=function(e){var t=c(e.components);return a.createElement(i.Provider,{value:t},e.children)},d="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},h=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,i=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),d=c(n),h=r,m=d["".concat(i,".").concat(h)]||d[h]||u[h]||o;return n?a.createElement(m,s(s({ref:t},p),{},{components:n})):a.createElement(m,s({ref:t},p))}));function m(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,s=new Array(o);s[0]=h;var l={};for(var i in t)hasOwnProperty.call(t,i)&&(l[i]=t[i]);l.originalType=e,l[d]="string"==typeof e?e:r,s[1]=l;for(var c=2;c<o;c++)s[c]=n[c];return a.createElement.apply(null,s)}return a.createElement.apply(null,n)}h.displayName="MDXCreateElement"},66670:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>i,contentTitle:()=>s,default:()=>d,frontMatter:()=>o,metadata:()=>l,toc:()=>c});var a=n(87462),r=(n(67294),n(3905));const o={title:"Kubernetes Constructs",sidebar_position:5},s=void 0,l={unversionedId:"kubernetes/constructs",id:"kubernetes/constructs",title:"Kubernetes Constructs",description:"The architecture",source:"@site/docs/kubernetes/constructs.md",sourceDirName:"kubernetes",slug:"/kubernetes/constructs",permalink:"/docs/kubernetes/constructs",draft:!1,tags:[],version:"current",sidebarPosition:5,frontMatter:{title:"Kubernetes Constructs",sidebar_position:5},sidebar:"tutorialSidebar",previous:{title:"the Kubernetes Scheduler",permalink:"/docs/kubernetes/scheduler"},next:{title:"Authentication, User Management, Authorization and Admission Control",permalink:"/docs/kubernetes/authentication-authorization"}},i={},c=[{value:"The architecture",id:"the-architecture",level:2},{value:"Master",id:"master",level:3},{value:"Node / minions",id:"node--minions",level:3},{value:"Core constructs",id:"core-constructs",level:2},{value:"Pods",id:"pods",level:3},{value:"Services",id:"services",level:3},{value:"Replication controllers",id:"replication-controllers",level:3},{value:"ReplicaSet",id:"replicaset",level:3},{value:"Health checks",id:"health-checks",level:2},{value:"Life cycle hooks or graceful shutdown",id:"life-cycle-hooks-or-graceful-shutdown",level:2},{value:"Application scheduling",id:"application-scheduling",level:2},{value:"Organizing the Cluster with Namespaces, Labels, and Annotations",id:"organizing-the-cluster-with-namespaces-labels-and-annotations",level:2},{value:"Namespaces",id:"namespaces",level:3},{value:"Labels and Label Selector",id:"labels-and-label-selector",level:3},{value:"Annotations",id:"annotations",level:3}],p={toc:c};function d(e){let{components:t,...n}=e;return(0,r.kt)("wrapper",(0,a.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h2",{id:"the-architecture"},"The architecture"),(0,r.kt)("p",null,"Kubernetes gives us automation and tooling to ensure high availability, application stack, and service-wide portability."),(0,r.kt)("p",null,"Kubernetes also allows finer control of resource usage, such as CPU, memory, and disk space across the infrastructure."),(0,r.kt)("p",null,"All the pieces of Kubernetes are constantly working to monitor the current actual state and synchronize it with the desired state defined by the administrators via the API server or ",(0,r.kt)("inlineCode",{parentName:"p"},"kubectl")," script."),(0,r.kt)("h3",{id:"master"},"Master"),(0,r.kt)("p",null,"In the ",(0,r.kt)("strong",{parentName:"p"},"master")," node, we have the core API server, which maintains RESTful web services for querying and defining our desired cluster and workload state. It's important to note that the control pane only accesses the master to initiate changes and not the nodes directly."),(0,r.kt)("p",null,"Additionally, the master includes the ",(0,r.kt)("strong",{parentName:"p"},"scheduler"),", which works with the API server to schedule workloads in the form of pods on the actual minion nodes."),(0,r.kt)("p",null,"The ",(0,r.kt)("strong",{parentName:"p"},"replication controller/replica set")," works with the API server to ensure that the correct number of pod replicas are running at any given time."),(0,r.kt)("p",null,"Also, we have ",(0,r.kt)("inlineCode",{parentName:"p"},"etcd")," running as a distributed configuration store. The Kubernetes state is stored here and ",(0,r.kt)("inlineCode",{parentName:"p"},"etcd")," allows values to be watched for changes."),(0,r.kt)("h3",{id:"node--minions"},"Node / minions"),(0,r.kt)("p",null,"In each node, we have a couple of components."),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"kubelet")," interacts with the API server to update the state and to start new workloads that have been invoked by the scheduler."),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"kube-proxy")," provides basic load balancing and directs the traffic destined for specific services to the proper pod on the backend."),(0,r.kt)("p",null,"Also, there are some default pods running various infrastructure services for each node, including DNS, logging, health checks."),(0,r.kt)("h2",{id:"core-constructs"},"Core constructs"),(0,r.kt)("p",null,"These abstractions will make it easier to think about our applications and ease the burden of life cycle management, high availability, and scheduling."),(0,r.kt)("h3",{id:"pods"},"Pods"),(0,r.kt)("p",null,"Pods allow you to keep related containers close in terms of the network and hardware infrastructure. Pods essentially allow you to logically group containers and pieces of our application stacks together."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Data can live near the application, so processing can be done without incurring a high latency from network traversal. Similarly, common data can be stored on volumes that are shared between a number of containers.")),(0,r.kt)("p",null,"Pods give us a logical group of containers that we can then replicate, schedule, and balance service endpoints across. If the process in a container crashes, Kubernetes automatically restarts it."),(0,r.kt)("p",null,"For example, they all share the same network namespace, which means that each container in a Pod can see the other containers in the Pod on ",(0,r.kt)("inlineCode",{parentName:"p"},"localhost"),". Pods also share the process and interprocess communication namespaces so that different containers can use tools, like shared memory and signaling, to coordinate between the different processes in the Pod."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Keeping the container images separate generally makes it more agile for different teams to own or reuse the container images, but grouping them together in a Pod at runtime enables them to operate cooperatively.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: v1\nkind: Pod\nmetadata:\n  name: kuard\nspec:\n  volumes:\n    - name: 'kuard-data'\n      hostPath:\n        path: '/var/lib/kuard'\n    - name: 'kuard-remote-data'\n      nfs:\n        server: my.nfs.server.local\n        path: '/exports'\n  containers:\n    - image: kuard-amd64:blue\n    name: kuard\n    volumeMounts:\n      - mountPath: '/data'\n        name: 'kuard-data'\n    resources:\n      requests: # minimum\n        cpu: '500m'\n        memory: '128Mi'\n      limits: # maximum\n        cpu: '1000m'\n        memory: '256Mi'\n    livenessProbe:\n      httpGet:\n        path: /healthy\n        port: 8080\n      initialDelaySeconds: 5\n      timeoutSeconds: 1\n      periodSeconds: 10\n      failureThreshold: 3\n    readinessProbe:\n      httpGet:\n        path: /ready\n        port: 8080\n      initialDelaySeconds: 30\n      timeoutSeconds: 1\n      periodSeconds: 10\n      failureThreshold: 3\n    ports:\n      - containerPort: 8080\n        name: http\n        protocol: TCP\n")),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Resources are requested per container, not per Pod. The total resources requested by the Pod is the sum of all resources requested by all containers in the Pod. The reason for this is that in many cases the different containers have very different CPU requirements.")),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Once a Pod is scheduled to a node, no rescheduling occurs if that node fails. Additionally, to create multiple replicas of the same Pod you have to create and name them manually. (That's why ReplicaSet comes into play.)")),(0,r.kt)("h3",{id:"services"},"Services"),(0,r.kt)("p",null,"A ",(0,r.kt)("inlineCode",{parentName:"p"},"Service")," represents a TCP or UDP load-balanced service, spreading traffic to these different Pod replicas."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Using a reliable endpoint, users and other programs can access pods running on your cluster seamlessly.")),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"the ",(0,r.kt)("inlineCode",{parentName:"p"},"kube-proxy")," watches for new services in the cluster via the API server. It then programs a set of ",(0,r.kt)("em",{parentName:"p"},"iptables")," rules in the kernel of that host to rewrite the destinations of packets so they are directed at one of the endpoints for that service. If the set of endpoints for a service changes (due to Pods coming and going or due to a failed readiness check), the set of ",(0,r.kt)("em",{parentName:"p"},"iptables")," rules is rewritten.")),(0,r.kt)("p",null,"Every Service that is created, whether TCP or UDP, gets three things:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Its own virtual but fixed IP address, VIP, also called cluster IP"),(0,r.kt)("li",{parentName:"ul"},"A DNS entry in the Kubernetes cluster DNS"),(0,r.kt)("li",{parentName:"ul"},"Load-balancing rules that proxy traffic to the Pods that implement the Service")),(0,r.kt)("p",null,"The Service object does is track which of your Pods are ready via a readiness check."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Only ready Pods are sent traffic.")),(0,r.kt)("p",null,"Kubernetes achieves this by making sure that every node in the cluster runs a proxy named ",(0,r.kt)("inlineCode",{parentName:"p"},"kube-proxy"),". The job of ",(0,r.kt)("inlineCode",{parentName:"p"},"kube-proxy")," is to proxy communication from a service endpoint back to the corresponding pod that is running the actual application."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Updates to service definitions are monitored and coordinated from the Kubernetes cluster master and propagated to the ",(0,r.kt)("inlineCode",{parentName:"p"},"kube-proxy")," daemons running on each node.")),(0,r.kt)("p",null,"The service could use label selectors for pod membership."),(0,r.kt)("p",null,"The Service load balancing is programmed into the network fabric of the Kubernetes cluster so that any container that tries to talk to the Service IP address is correctly load balanced to the corresponding Pods."),(0,r.kt)("p",null,"This programming of the network fabric is dynamic, so as Pods come and go due to failures or scaling of a ReplicaSet, the load balancer is constantly reprogrammed to match the current state of the cluster."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Importing External Services")),(0,r.kt)("p",null,"We might have legacy server and service are not running directly in Kubernetes, but it's still worthwhile to represent this server in Kubernetes."),(0,r.kt)("details",null,(0,r.kt)("summary",null,(0,r.kt)("p",null,"To see concretely how you maintain high fidelity between ",(0,r.kt)("em",{parentName:"p"},"development")," and ",(0,r.kt)("em",{parentName:"p"},"production"),", remember that all Kubernetes objects are deployed into ",(0,r.kt)("inlineCode",{parentName:"p"},"namespaces"),".")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"# service in test\nkind: Service\nmetadata:\n  name: my-database\n  namespace: test\n---\n# service in production\nkind: Service\nmetadata:\n  name: my-database\n  namespace: prod\n")),(0,r.kt)("p",null,"When you deploy a Pod into the ",(0,r.kt)("inlineCode",{parentName:"p"},"test")," namespace and it looks up the service named ",(0,r.kt)("inlineCode",{parentName:"p"},"my-database"),", it will receive a pointer to ",(0,r.kt)("inlineCode",{parentName:"p"},"my-database.test.svc.cluster.internal"),", which in turn points to the test database."),(0,r.kt)("p",null,"In contrast, when a Pod deployed in the ",(0,r.kt)("inlineCode",{parentName:"p"},"prod")," namespace looks up the same name (",(0,r.kt)("inlineCode",{parentName:"p"},"my-database"),") it will receive a pointer to ",(0,r.kt)("inlineCode",{parentName:"p"},"my-database.prod.svc.cluster.internal"),", which is the production database.")),(0,r.kt)("details",null,(0,r.kt)("summary",null,(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Service without selectors")),(0,r.kt)("p",null,"With external services, however, there is no such label query.")),(0,r.kt)("p",null,"Instead, you generally have a DNS name that points to the specific server running the database. To import this external database service into Kubernetes, we start by creating a service without a Pod selector that references the DNS name of the database server."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"kind: Service\napiVersion: v1\nmetadata:\n  name: external-database\nspec:\n  type: ExternalName\n  externalName: database.company.com\n")),(0,r.kt)("p",null,"When a typical Kubernetes service is created, an IP address is also created and the Kubernetes DNS service is populated with an A record that points to that IP address."),(0,r.kt)("p",null,"When you create a service of type ",(0,r.kt)("inlineCode",{parentName:"p"},"ExternalName"),", the Kubernetes DNS service is instead populated with a CNAME record that points to the external name you specified (database.company.com in this case)."),(0,r.kt)("p",null,"When an application in the cluster does a DNS lookup for the hostname ",(0,r.kt)("inlineCode",{parentName:"p"},"external-database.svc.default.cluster"),", the DNS protocol aliases that name to ",(0,r.kt)("inlineCode",{parentName:"p"},"database.company.com"),". This then resolves to the IP address of your external database server."),(0,r.kt)("p",null,"In this way, all containers in Kubernetes believe that they are talking to a service that is backed with other containers, when in fact they are being redirected to the external database."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"# external-ip-service.yaml\nkind: Service\napiVersion: v1\nmetadata:\n  name: external-ip-database\n")),(0,r.kt)("p",null,"At this point, Kubernetes will allocate a virtual IP address for this service and populate an A record for it. However, because there is no selector for the service, there will be no endpoints populated for the load balancer to redirect traffic to."),(0,r.kt)("p",null,"Given that this is an external service, the user is responsible for populating the endpoints manually with an ",(0,r.kt)("inlineCode",{parentName:"p"},"Endpoints")," resource."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"external-ip-endpoints.yaml\nkind: Endpoints\napiVersion: v1\nmetadata:\n  name: external-ip-database\nsubsets:\n  - addresses:\n    - ip: 192.168.0.1\n      ports:\n        - port: 3306\n")),(0,r.kt)("p",null,"If you have more than one IP address for redundancy, you can repeat them in the addresses array. Once the endpoints are populated, the load balancer will start redirecting traffic from your Kubernetes service to the IP address endpoint(s).")),(0,r.kt)("h3",{id:"replication-controllers"},"Replication controllers"),(0,r.kt)("p",null,"Replication controllers (RCs) manage the number of nodes that a pod and included container images run on. They ensure that an instance of an image is being run with the specific number of copies."),(0,r.kt)("p",null,"Replication controllers are simply charged with ensuring that you have the desired scale for your application."),(0,r.kt)("details",null,(0,r.kt)("summary",null,(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Example configuration file"))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: v1 # schema version\nkind: ReplicationController #  the type of resource\nmetadata:\n  name: node-js # the name of the resource\n  labels: # labels which will be used for searching/selecting\n    name: node-js\nspec:\n  replicas: 3 # number of pods\n  selector:\n    #  the selector values need to match the labels values specified in the pod template.\n    name: node-js #  tells the controller which pods to watch\n  template: # a template to launch a new pod\n    metadata:\n      labels:\n        name: node-js\n    spec:\n      containers:\n        - name: node-js\n          image: jonbaier/node-express-info:latest\n          ports:\n            - containerPort: 80\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: v1\nkind: Service\nmetadata:\n  name: node-js\n  labels:\n    name: node-js\nspec:\n  type: LoadBalancer\n  ports:\n    - port: 80\n    selector:\n      name: node-js\n"))),(0,r.kt)("h3",{id:"replicaset"},"ReplicaSet"),(0,r.kt)("p",null,"A ",(0,r.kt)("inlineCode",{parentName:"p"},"ReplicaSet")," ensures that, for a given Pod definition, a number of replicas exists within the system. The actual replication is handled by the Kubernetes controller manager, which creates Pod objects that are scheduled by the Kubernetes scheduler."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Also, the usage of ReplicationController is strongly discouraged in favor of ReplicaSets.")),(0,r.kt)("details",null,(0,r.kt)("summary",null,(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Example configuration file"))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: extensions/v1beta1\nkind: ReplicaSet\nmetadata:\n  # required unique field\n  name: node-js-rs\nspec:\n  replicas: 3\n  selector:\n    # The selector should be a proper subset\n    # of the labels in the Pod template.\n    matchLabels:\n      app: node-js-express\n      deployment: test\n    matchExpressions:\n      - { key: name, operator: In, values: [node-js-rs] }\n\n  # Pod template\n  template:\n    metadata:\n      labels:\n        name: node-js-rs\n        app: node-js-express\n        deployment: test\n        version: v1\n    spec:\n      containers:\n        - name: node-js-rs\n          image: jonbaier/node-express-info:latest\n          ports:\n            - containerPort: 80\n")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Reconciliation Loops")),(0,r.kt)("p",null,"The central concept behind a reconciliation loop is the notion of ",(0,r.kt)("em",{parentName:"p"},"desired state")," versus ",(0,r.kt)("em",{parentName:"p"},"observed or current state"),"."),(0,r.kt)("p",null,"The reconciliation loop is constantly running, observing the current state of the world and taking action to try to make the observed state match the desired state."),(0,r.kt)("p",null,"Though ReplicaSets create and manage Pods, they do not own the Pods they create."),(0,r.kt)("p",null,"ReplicaSets use label queries to identify the set of Pods they should be managing. They then use the Pod API to create the Pods that they are managing."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Designing with ReplicaSets")),(0,r.kt)("p",null,"ReplicaSets are designed to represent a single, scalable microservice inside your architecture."),(0,r.kt)("p",null,"Typically, these Pods created by ReplicaSet are then fronted by a Kubernetes service load balancer, which spreads traffic across the Pods that make up the service."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Generally speaking, ReplicaSets are designed for stateless (or nearly stateless) services."))),(0,r.kt)("h2",{id:"health-checks"},"Health checks"),(0,r.kt)("p",null,"Kubernetes provides two layers of health checking."),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"HTTP or TCP checks:"),(0,r.kt)("p",{parentName:"li"},"Kubernetes can attempt to connect to a particular endpoint and give a status of healthy on a successful connection.")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Application-specific health checks can be performed using command-line scripts."))),(0,r.kt)("details",null,(0,r.kt)("summary",null,(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Example configuration file"))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: node-js\n  labels:\n    name: node-js\nspec:\n  replicas: 3\n  selector:\n    name: node-js\n  template:\n    metadata:\n      labels:\n        name: node-js\n    spec:\n      containers:\n        - name: node-js\n          image: jonbaier/node-express-info:latest\n          ports:\n            - containerPort: 80\n          livenessProbe:\n            # An HTTP health check\n            httpGet:\n              path: /status/\n              port: 80\n            initialDelaySeconds: 30\n            timeoutSeconds: 1\n\n          readinessProbe:\n            # An HTTP health check\n            httpGet:\n              path: /status/\n              port: 80\n            initialDelaySeconds: 30\n            timeoutSeconds: 1\n\n          livenessProbe:\n            # An TCP health check\n            tcpSocket:\n              port: 80\n            initialDelaySeconds: 15\n            timeoutSeconds: 1\n\n          livenessProbe:\n            exec:\n              command:\n              - /usr/bin/health/checkHttpService.sh\n            initialDelaySeconds: 90\n            timeoutSeconds: 1\n")),(0,r.kt)("p",null,"Note the addition of the ",(0,r.kt)("inlineCode",{parentName:"p"},"livenessProbe")," element. This is our core health check element. From here, we can specify ",(0,r.kt)("inlineCode",{parentName:"p"},"httpGet"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"tcpScoket"),", or ",(0,r.kt)("inlineCode",{parentName:"p"},"exec"),"."),(0,r.kt)("p",null,"The probe will check the path and port specified and ",(0,r.kt)("em",{parentName:"p"},"restart")," the pod if it doesn't successfully return."),(0,r.kt)("p",null,"There is a separate ",(0,r.kt)("inlineCode",{parentName:"p"},"readinessProbe")," that will remove a container from the pool of pods answering service endpoints.")),(0,r.kt)("h2",{id:"life-cycle-hooks-or-graceful-shutdown"},"Life cycle hooks or graceful shutdown"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/"},"Container Lifecycle Hooks")),(0,r.kt)("p",null,"As you run into failures in real-life scenarios, you may find that you want to take additional action before containers are shutdown or right after they are started. Kubernetes actually provides life cycle hooks for just this kind of use case."),(0,r.kt)("details",null,(0,r.kt)("summary",null,(0,r.kt)("p",null,"The following example controller definition defines both a ",(0,r.kt)("inlineCode",{parentName:"p"},"postStart")," action and a ",(0,r.kt)("inlineCode",{parentName:"p"},"preStop")," action to take place before Kubernetes moves the container into the next stage of its life cycle."),(0,r.kt)("p",null,"It's important to note that hook calls are delivered at least once. Therefore, any logic in the action should gracefully handle multiple calls."),(0,r.kt)("p",null,"Another important note is that ",(0,r.kt)("inlineCode",{parentName:"p"},"postStart")," runs before a pod enters its ready state. If the hook itself fails, the pod will be considered unhealthy.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: apache-hook\n  labels:\n    name: apache-hook\nspec:\n  replicas: 3\n  selector:\n    name: apache-hook\n  template:\n    metadata:\n      labels:\n        name: apache-hook\n    spec:\n      containers:\n        - name: apache-hook\n          image: bitnami/apache:latest\n          ports:\n            - containerPort: 80\n          lifecycle:\n            postStart:\n              # make an HTTP call to the endpoint and port\n              httpGet:\n                path: http://my.registration-server.com/register/\n                port: 80\n\n            preStop:\n              # runs a local command in the container\n              # a parameter named `reason` will be sent to the handler as a parameter\n              exec:\n                command: ['/usr/local/bin/apachectl', '-k', 'graceful-stop']\n"))),(0,r.kt)("h2",{id:"application-scheduling"},"Application scheduling"),(0,r.kt)("p",null,"The default behavior for the Kubernetes scheduler is to spread container replicas across the nodes in the cluster. In the absence of all other constraints, the scheduler will place new pods on nodes with the least number of other pods belonging to matching services or replication controllers."),(0,r.kt)("p",null,"Additionally, the scheduler provides the ability to add constraints based on resources available to the node. Today, this includes minimum CPU and memory allocations."),(0,r.kt)("p",null,"When additional constraints are defined, Kubernetes will check a node for available resources. If a node does not meet all the constraints, it will move to the next. If no nodes can be found that meet the criteria, then we will see a scheduling error in the logs."),(0,r.kt)("details",null,(0,r.kt)("summary",null,(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Example configuration file"),": requesting ",(0,r.kt)("em",{parentName:"p"},"512 Mi")," for memory and ",(0,r.kt)("em",{parentName:"p"},"1500 m")," for CPU.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: node-js-constraints\n  labels:\n    name: node-js-constraints\nspec:\n  replicas: 3\n  selector:\n    name: node-js-constraints\n  template:\n    metadata:\n      labels:\n        name: node-js-constraints\n    spec:\n      containers:\n        - name: node-js-constraints\n          image: jonbaier/node-express-info:latest\n          ports:\n            - containerPort: 80\n          resources:\n            limits:\n              memory: '512Mi'\n              cpu: '1500m' # '500m'\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl apply -f nodejs-constraints-controller.yml\n\nkubectl get pods\n\nkubectl describe pods/<pod-id>\n"))),(0,r.kt)("h2",{id:"organizing-the-cluster-with-namespaces-labels-and-annotations"},"Organizing the Cluster with Namespaces, Labels, and Annotations"),(0,r.kt)("h3",{id:"namespaces"},"Namespaces"),(0,r.kt)("p",null,"You can think of a ",(0,r.kt)("inlineCode",{parentName:"p"},"Namespace")," as something like a folder for your Kubernetes API objects."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Namespaces provide directories for containing most of the other objects in the cluster. Namespaces can also provide a scope for role-based access control (RBAC) rules.")),(0,r.kt)("p",null,"Every Kubernetes cluster has a single built-in Namespace named ",(0,r.kt)("inlineCode",{parentName:"p"},"default"),", and most installations of Kubernetes also include a Namespace named ",(0,r.kt)("inlineCode",{parentName:"p"},"kube-system"),", where cluster administration containers are created."),(0,r.kt)("p",null,"Namespaces are also placed into the DNS names created for Services and the DNS search paths that are provided to containers."),(0,r.kt)("h3",{id:"labels-and-label-selector"},"Labels and Label Selector"),(0,r.kt)("p",null,"Labels provide identifying metadata for objects. These are fundamental qualities of the object that will be used for grouping, viewing, and operating."),(0,r.kt)("p",null,"Labels have simple syntax. They are key/value pairs, where both the key and value are represented by strings. Label keys can be broken down into two parts: an optional prefix and a name, separated by a slash. The prefix, if specified, must be a DNS sub\u2010 domain with a 253-character limit. The key name is required and must be shorter than 63 characters. Names must also start and end with an alphanumeric character and permit the use of dashes (-), underscores (","_","), and dots (.) between characters."),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:"left"},"Key"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Value"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"example.com/app-version"),(0,r.kt)("td",{parentName:"tr",align:"left"},"1.0.0")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"appVersion"),(0,r.kt)("td",{parentName:"tr",align:"left"},"1.0.0")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"kubernetes.io/cluster-service"),(0,r.kt)("td",{parentName:"tr",align:"left"},"true")))),(0,r.kt)("p",null,"Label selectors are used to filter Kubernetes objects based on a set of labels. Selectors use a simple Boolean language."),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:"left"},"Operator"),(0,r.kt)("th",{parentName:"tr",align:"left"},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"key=value"),(0,r.kt)("td",{parentName:"tr",align:"left"},"key is set to value")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"key!=value"),(0,r.kt)("td",{parentName:"tr",align:"left"},"key is not set to value")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"key in (value1, value2)"),(0,r.kt)("td",{parentName:"tr",align:"left"},"key is one of (...)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"key notin (value1, value2)"),(0,r.kt)("td",{parentName:"tr",align:"left"},"key is not one of (...)")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"key"),(0,r.kt)("td",{parentName:"tr",align:"left"},"key is set")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:"left"},"!key"),(0,r.kt)("td",{parentName:"tr",align:"left"},"key is not set")))),(0,r.kt)("h3",{id:"annotations"},"Annotations"),(0,r.kt)("p",null,"Annotations provide a place to store additional metadata for Kubernetes objects with the sole purpose of assisting tools and libraries. They are a way for other programs driving Kubernetes via an API to store some opaque data with an object."),(0,r.kt)("p",null,"Annotations are used in various places in Kubernetes, with the primary use case being rolling deployments. During rolling deployments, annotations are used to track rollout status and provide the necessary information required to roll back a deployment to a previous state."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"It is a matter of taste as to when to use an annotation or a label. When in doubt, add information to an object as an annotation and pro\u2010 mote it to a label if you find yourself wanting to use it in a selector.")))}d.isMDXComponent=!0}}]);