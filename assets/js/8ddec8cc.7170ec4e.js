"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[29],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>p});var r=n(67294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},i=Object.keys(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=r.createContext({}),c=function(e){var t=r.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},u=function(e){var t=c(e.components);return r.createElement(l.Provider,{value:t},e.children)},h="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},d=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,i=e.originalType,l=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),h=c(n),d=a,p=h["".concat(l,".").concat(d)]||h[d]||m[d]||i;return n?r.createElement(p,o(o({ref:t},u),{},{components:n})):r.createElement(p,o({ref:t},u))}));function p(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=n.length,o=new Array(i);o[0]=d;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[h]="string"==typeof e?e:a,o[1]=s;for(var c=2;c<i;c++)o[c]=n[c];return r.createElement.apply(null,o)}return r.createElement.apply(null,n)}d.displayName="MDXCreateElement"},74452:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>i,metadata:()=>s,toc:()=>c});var r=n(87462),a=(n(67294),n(3905));const i={title:"Concurrency at Scale",sidebar_position:5},o=void 0,s={unversionedId:"concurrency-in-go/scalability",id:"concurrency-in-go/scalability",title:"Concurrency at Scale",description:"Scaling concurrent operations within a single process.",source:"@site/docs/concurrency-in-go/scalability.md",sourceDirName:"concurrency-in-go",slug:"/concurrency-in-go/scalability",permalink:"/docs/concurrency-in-go/scalability",draft:!1,tags:[],version:"current",sidebarPosition:5,frontMatter:{title:"Concurrency at Scale",sidebar_position:5},sidebar:"tutorialSidebar",previous:{title:"Concurrency Patterns in Go",permalink:"/docs/concurrency-in-go/patterns"},next:{title:"Goroutines and the Go Runtime",permalink:"/docs/concurrency-in-go/internal"}},l={},c=[{value:"Error Propagation",id:"error-propagation",level:2},{value:"Timeouts and Cancellation",id:"timeouts-and-cancellation",level:2},{value:"Heartbeats",id:"heartbeats",level:2},{value:"Replicated Requests",id:"replicated-requests",level:2},{value:"Rate Limiting",id:"rate-limiting",level:2},{value:"Healing Unhealthy Goroutines",id:"healing-unhealthy-goroutines",level:2}],u={toc:c};function h(e){let{components:t,...n}=e;return(0,a.kt)("wrapper",(0,r.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("p",null,"Scaling concurrent operations within a single process."),(0,a.kt)("p",null,"Finding out how concurrency comes into play when dealing with more than one process."),(0,a.kt)("h2",{id:"error-propagation"},"Error Propagation"),(0,a.kt)("p",null,"With concurrent code, and especially distributed systems, it\u2019s both easy for something to go wrong in your system, and difficult to understand why it happened."),(0,a.kt)("p",null,"Let\u2019s spend some time here discussing a philosophy of error propagation. What follows is an opinionated framework for handling errors in concurrent systems."),(0,a.kt)("p",null,"Errors indicate that your system has entered a state in which it cannot fulfill an operation. Because of this, it needs to relay a few pieces of critical information:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"What happened:"),(0,a.kt)("p",{parentName:"li"},"This is the part of the error that contains information about what happened. This information is likely to be generated implicitly by whatever it was that generated the errors, although you can probably decorate this with some context that will help the user.")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"When and where it occurred:"),(0,a.kt)("p",{parentName:"li"},"Errors should always contain a complete stack trace starting with how the call was initiated and ending with where the error was instantiated. The stack trace should not be contained in the error message, but should be easily accessible when handling the error up the stack."),(0,a.kt)("p",{parentName:"li"},"Further, the error should contain information regarding the context it\u2019s running within, as well as the time on the machine the error was instantiated on, in UTC.")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"A friendly user-facing message, optional."),(0,a.kt)("p",{parentName:"li"},"The message that gets displayed to the user should be customized to suit your system and its users. It should only contain abbreviated and relevant information from the previous two points.")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"How the user can get more information, optional."),(0,a.kt)("p",{parentName:"li"},"At some point, someone will likely want to know, in detail, what happened when the error occurred. Errors that are presented to users should provide an ID that can be cross-referenced to a corresponding log that displays the full information of the error: time the error occurred (not the time the error was logged), the stack trace. It can also be helpful to include a hash of the stack trace to aid in aggregating like issues in bug trackers."))),(0,a.kt)("p",null,"It\u2019s possible to place all errors into one of two categories:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Bugs")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Known edge cases (e.g., broken network connections, failed disk writes, etc.)"))),(0,a.kt)("p",null,"Bugs are errors that the program have not customized to the system, or ",(0,a.kt)("em",{parentName:"p"},"raw errors"),". Imagine a large system with multiple modules:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"  CLI Component --\x3e Intermediary Component --\x3e Low Level Component\n")),(0,a.kt)("p",null,"At boundaries of each component, all incoming errors must be wrapped in a well-formed error for the component code is within. Sometimes we need to wrap error to be malformed."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-go"},'func PostReport(id string) error {\n  result, err := lowlevel.DoWork()\n  if err != nil {\n    if _, ok := err.(lowlevel.Error); ok {\n      err = WrapErr(err, "cannot post report with id %q", id)\n    }\n    return err\n  }\n}\n\ntype MyError struct {\n  Inner error\n  Message string\n  StackTrace string\n  Misc map[string]interface{}\n}\n\nfunc wrapError(err error, messagef string, msgArgs ...interface{}) MyError {\n  return MyError{\n    Inner: err,\n    Message: fmt.Sprintf(messagef, msgArs...),\n    StackTrace: string(debug.Stack()),\n    Misc: make(map[string]interface{}),\n  }\n}\n\nfunc (err MyError) Error() string {\n  return err.Message\n}\n\ntype LowLevelErr struct {\n  error\n}\n\nfunc isGloballyExec(path string) (bool, error) {\n  info, err := os.Stat(path)\n  if err != nil {\n    return false, LowLevelErr{wrapError(err, err.Error())}\n  }\n  return info.Mode().Perm()&0100 = 0100, nil\n}\n\ntype IntermediateErr struct {\n  error\n}\n')),(0,a.kt)("p",null,"Note that it is only necessary to wrap errors in the module boundaries, such as public functions/methods, or when the code can add valuable context."),(0,a.kt)("p",null,"As we printing these errors with much information, we should display a friendly message to the user stating something unexpected has happened."),(0,a.kt)("h2",{id:"timeouts-and-cancellation"},"Timeouts and Cancellation"),(0,a.kt)("p",null,"Timeouts are crucial to creating a system with behavior you can understand. Cancellation is one natural response to a timeout."),(0,a.kt)("p",null,"Here are some reasons we might want the concurrent processes to support timeouts:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"System saturation"),(0,a.kt)("p",{parentName:"li"},"If the system is saturated, it may want the requests at the edges of the system to time out rather than take a long time to field them.")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Stale data"),(0,a.kt)("p",{parentName:"li"},"Sometimes data has a window within which it must be processed before more relevant data is available, or the need to process the data has expired. If a concurrent process takes longer to process the data than this window, we would want to time out and cancel the concurrent process."),(0,a.kt)("p",{parentName:"li"},"If this window is known beforehand, it would make sense to pass our concurrent process a ",(0,a.kt)("inlineCode",{parentName:"p"},"context.Context")," created with ",(0,a.kt)("inlineCode",{parentName:"p"},"context.WithDeadline"),", or ",(0,a.kt)("inlineCode",{parentName:"p"},"context.WithTimeout"),". If the window is not known beforehand, the parent of the concurrent process could use ",(0,a.kt)("inlineCode",{parentName:"p"},"context.WithCancel")," to be able to cancel the concurrent process when there's no more need for the request.")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Attempting to prevent deadlocks"),(0,a.kt)("p",{parentName:"li"},"In a large system\u2014especially distributed systems\u2014it can sometimes be difficult to understand the way in which data might flow, or what edge cases might turn up. It is not unreasonable, and even recommended, to place timeouts on all of your concurrent operations to guarantee your system won't deadlock. The timeout period doesn't have to be close to the actual time it takes to perform your concurrent operation. The timeout period\u2019s purpose is only to prevent deadlock, choose a reasonable time duration."))),(0,a.kt)("p",null,"Here are a number of reasons why a concurrent process might be canceled:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Timeouts"),(0,a.kt)("p",{parentName:"li"},"A timeout is an implicit cancellation.")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"User intervention"),(0,a.kt)("p",{parentName:"li"},"It's usually advisable to start long-running processes concurrently and then report status back to the user at a polling interval, or allow the users to query for status as they see fit. So it is necessary to allow the users to cancel the operation they've started.")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Parent cancellation"),(0,a.kt)("p",{parentName:"li"},"For that matter, if any kind of parent of a concurrent operation stops, as a child of that parent, it should be canceled.")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Replicated Requests"),(0,a.kt)("p",{parentName:"li"},"Sometimes, we may send data to multiple concurrent processes in an attempt to get a faster response from one of them. When the first one comes back, we would want to cancel the rest of the processes."))),(0,a.kt)("p",null,"To reify cancellation, we need to explore the preemptability of a concurrent process."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-go"},"val value interface{}\nselect {\ncase <-done:\n  return\ncase value = <-valueStream:\n}\n\nresult := reallyLongCalculation(value)\n\nselect {\ncase <-done:\n  return\ncase valueStream<-result:\n}\n\n// reallyLongCalculation is preemptable\nreallyLongCalculation := func(\n  done <-chan interface{},\n  value interface{},\n) interface{} {\n  intermediateResult := longCalculation(done, value)\n  return logCalculation(done, intermediateResult)\n}\n")),(0,a.kt)("p",null,"Using cancellation lurking another problem: if the goroutine happens to modify shared state what happens when the goroutine is canceled? Does the goroutine try and roll back the intermediary work it\u2019s done? How long does it have to do this work? Something has told the goroutine that it should halt, so the goroutine shouldn't take too long to roll back its work, right?"),(0,a.kt)("p",null,"Backing out or rolling back may be required after a cancellation."),(0,a.kt)("p",null,"Another issue is duplicated message/result. Both the parent and the child can take their own strategy to response the duplicated result:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"The child accepts either the first or the last result")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"The child poll the parent for permission to deal with result"))),(0,a.kt)("h2",{id:"heartbeats"},"Heartbeats"),(0,a.kt)("p",null,"Heartbeats are a way for concurrent processes to signal life to outside parties. There are two different types of heartbeats discussed below:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Heartbeats that occur on a time interval")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Heartbeats that occur at the beginning of a unit of work"))),(0,a.kt)("p",null,"Heartbeats that occur on a time interval are useful for concurrent code that might be waiting for something else to happen for it to process a unit of work. A heartbeat is a way to signal to its listeners that everything is well, and that the silence is expected."),(0,a.kt)("details",null,(0,a.kt)("summary",null,(0,a.kt)("p",null,"The following code demonstrates a goroutine that exposes a heartbeat:")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-go"},'doWork := func(\n  done <-chan interface{},\n  pulseInterval time.Duration,\n) (<-chan interface{}, <-chan item.Time) {\n  // set up a channel to send heartbeats on,\n  // return this out of doWork\n  heartbeat := make(chan interface{})\n  results := make(chan time.Time)\n  go func() {\n    defer close(heartbeat)\n    defer close(results)\n\n    pulse := time.Tick(pulseInterval)\n    workGen := time.Tick(2 * pulseInterval)\n\n    sendPulse := func() {\n      select {\n      case heartbeat <-struct{}{}:\n      default:\n        // guard against the fact no one may be\n        // listening to the heartbeat\n      }\n    }\n    sendResult := func(r time.Time) {\n      for {\n        select {\n        case <-done:\n          return\n        case <-pulse:\n          // just like with `done` channel\n          // anytime performing a send or receive,\n          // a case for the heartbeat\'s pulse is needed\n          sendPulse()\n        case results <- r:\n          return\n        }\n      }\n    }\n\n    for {\n      select {\n      case <-done:\n        return\n      case <-pulse:\n        // just like with `done` channel\n        // anytime performing a send or receive,\n        // a case for the heartbeat\'s pulse is needed\n        sendPulse()\n      case r := <-workGen:\n        sendResult(r)\n      }\n    }\n  }()\n  return heartbeat, results\n}\n\n// set up the standard `done` channel\ndone := make(chan interface{})\n// close the `done` channel after 10 seconds\ntime.AfterFunc(10 * time.Second, func() { close(done) })\n\nconst timeout = 2 * time.Second\nheartbeat, results := doWork(done, timeout / 2)\nfor {\n  select {\n  case _, ok := <-heartbeat:\n    // select on the heartbeat, if there is no result,\n    // at least guaranteed a message from the heartbeat channel\n    if ok == false {\n      return\n    }\n    fmt.Println("pulse")\n  case r, ok := <-results:\n    // select from the results channel\n    if ok == false {\n      return\n    }\n    fmt.Printf("results %v\\n", r.Second())\n  case <-time.After(timeout):\n    // here we timeout if we haven\'t\n    // received either a heartbeat or a new result\n    return\n  }\n}\n')),(0,a.kt)("p",null,"By using a heartbeat, we have successfully avoided a deadlock, and we remain deterministic by not having to rely on a longer timeout."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-go"},'doWork := func(\n  done <-chan interface{},\n  pulseInterval time.Duration,\n) (<-chan interface{}, <-chan item.Time) {\n  heartbeat := make(chan interface{})\n  results := make(chan time.Time)\n  go func() {\n    // do not close either channel\n    // after the end of this goroutine\n    pulse := time.Tick(pulseInterval)\n    workGen := time.Tick(2 * pulseInterval)\n\n    sendPulse := func() {\n      select {\n      case heartbeat <-struct{}{}:\n      default:\n      }\n    }\n    sendResult := func(r time.Time) {\n      for {\n        select {\n        case <-done:\n          return\n        case <-pulse:\n          sendPulse()\n        case results <- r:\n          return\n        }\n      }\n    }\n    // simulate a panic here,\n    // only loop twice\n    for i := 0; i < 2; i++ {\n      select {\n      case <-done:\n        return\n      case <-pulse:\n        sendPulse()\n      case r := <-workGen:\n        sendResult(r)\n      }\n    }\n  }()\n  return heartbeat, results\n}\n\ndone := make(chan interface{})\ntime.AfterFunc(10 * time.Second, func() { close(done) })\n\nconst timeout = 2 * time.Second\nheartbeat, results := doWork(done, timeout / 2)\nfor {\n  select {\n  case _, ok := <-heartbeat:\n    if ok == false {\n      return\n    }\n    fmt.Println("pulse")\n  case r, ok := <-results:\n    if ok == false {\n      return\n    }\n    fmt.Printf("results %v\\n", r.Second())\n  case <-time.After(timeout):\n    // rely on a longer timeout\n    fmt.Println("worker goroutine is not healthy")\n    return\n  }\n}\n'))),(0,a.kt)("details",null,(0,a.kt)("summary",null,(0,a.kt)("p",null,"Also, heartbeats can occur at the beginning of a unit of work. These are extremely useful for tests.")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-go"},"doWork := func(done <-chan interface{}) (<-chan interface{}, <-chan int) {\n  // create the heartbeat channel with a buffer of one.\n  // this ensures that there's always at least one pulse sent\n  // out even if no one is listening in time for the send to occur\n  heartbeatStream := make(chan interface{}, 1)\n  workStream := make(chan int)\n  go func() {\n    defer close(heartbeatStream)\n    defer close(workStream)\n\n    for i := 0; i < 10; i++ {\n      // set up a separate select block for the heartbeat\n      select {\n      case heartbeatStream <- struct{}{}:\n        // we don't include this in the same select block as\n        // the send on results because if the receiver isn't\n        // ready for the result, they'll receive a pulse instead\n        // and the current value of the result will be lost\n      default:\n        // guard against the fact that no one may be lisitening\n        // to the heartbeats.\n        // the heartbeats is created with a buffer of one, if\n        // someone is listening, but not in time for the first pulse,\n        // it will still be notified of a pulse\n      }\n\n      select {\n      case <-done:\n        return\n      case workStream <- rand.Intn(10):\n      }\n    }\n  }\n  return heartbeatStream, workStream\n}\n\ndone := make(chan interface{})\ndefer close(done)\n\nheartbeat, results := doWork(done)\nfor {\n  select {\n  case _, ok := <-heartbeat:\n    if ok {\n      fmt.Println(\"pulse\")\n    } else {\n      return\n    }\n  case r, ok := <-results:\n    if ok {\n      fmt.Printf(\"results %v\\n\", r)\n    } else {\n      return\n    }\n  }\n}\n"))),(0,a.kt)("details",null,(0,a.kt)("summary",null,(0,a.kt)("p",null,"Where this technique really shines is in writing tests. Interval-based heartbeats can be used in the same fashion, but if you only care that the goroutine has started doing its work, this style of heartbeat is simple.")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-go"},'func DoWork(\n  done <-chan interface{},\n  nums ...int,\n) (<-chan interface{}, <-chan int) {\n  heartbeat := make(chan interface{}, 1)\n  intStream := make(chan int)\n  go func() {\n    defer close(heartbeat)\n    defer close(intStream)\n    // simulate some kind of delay before\n    // the goroutine can begin working\n    time.Sleep(2 * time.Second)\n\n    for _, n := range nums {\n      select {\n      case heartbeat <- struct{}{}:\n      default:\n      }\n\n      select {\n      case <-done:\n        return\n      case intStream <-n:\n      }\n    }\n  }()\n\n  return heartbeat, intStream\n}\n\n/*\n  This test is bad because it\'s nondeterministic.\n  We cannot guaranteed that the first iteration of the goroutine\n  will occur before the timeout\n*/\nfunc TestDoWork_GeneratesAllNumbers(t *testing.T) {\n  done := make(chan interface{})\n  defer close(done)\n\n  intSlice := []int{0, 1, 3, 2, 5}\n  _, results := DoWork(done, intSlice...)\n\n  for i, expected := range intSlice {\n    select {\n    case r := <-results:\n      if r != expected {\n        t.Errorf(\n          "index %v: expected %v, but received %v,"\n          i,\n          expected,\n          r,\n        )\n      }\n    case <-time.After(1 * time.Second):\n      t.Fatal("test timed out")\n    }\n  }\n}\n\n/*\n  Using heartbeat, we can solve this problem\n  Here is a good and deterministic test.\n  The only risk is one of the iterations taking an\n  inordinate amount of time.\n*/\nfunc TestDoWork_GenerateAllNumbers(t *testing.T) {\n  done := make(chan interface{})\n  defer close(done)\n\n  intSlice := []int{0, 1, 3, 2, 5}\n  heartbeat, results := DoWork(done, intSlice...)\n\n  // we wait for the goroutine to signal that\n  // it\'s beginning to process an iteration, aka results working\n  <-heartbeat\n\n  i := 0\n  for r := range results {\n    if expected := intSlice[i]; r != expected {\n      t.Errorf(\n        "index %v: expected %v, but received %v",\n        i,\n        expected,\n        r,\n      )\n    }\n    i++\n  }\n}\n\n'))),(0,a.kt)("h2",{id:"replicated-requests"},"Replicated Requests"),(0,a.kt)("p",null,"If this replication is done in-memory, it might not be that costly, but if replicating the handlers requires replicating processes, servers, or even data centers, this can become quite costly."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-go"},'doWork := func(\n  done <-chan interface{},\n  id int,\n  wg *sync.WaitGroup,\n  result chan<-int,\n) {\n  started := time.Now()\n  defer wg.Done()\n\n  simulatedLoadTime := time.Duration(1+rand.Intn(5)) * time.Second\n  fmt.Printf("%v would take %v\\n", id, simulatedLoadTime)\n  select {\n  case <-done:\n  case <-time.After(simulatedLoadTime):\n  }\n\n  select {\n  case <-done:\n  case result<-id:\n  }\n\n  took := time.Since(started)\n  // Display how long handlers would have taken\n  if took < simulatedLoadTime {\n    took = simulatedLoadTime\n  }\n  fmt.Printf("%v took %v\\n", id, took)\n}\n\ndone := make(chan interface{})\nresult := make(chan int)\n\nvar wg sync.WaitGroup\nwg.Add(10)\n\nfor i := 0; i < 10; i++ {\n  go doWork(done, i, &wg, result)\n}\n\nfirstReturned := <-result\n// cancel the remaining handlers\nclose(done)\nwg.Wait()\nfmt.Printf("Received an answer from #%v\\n", firstReturned)\n')),(0,a.kt)("p",null,"The only caveat to this approach is that all handlers need to have equal opportunity to service the request. In other words, the program is not going to have a chance at receiving the fastest time from a handler that can't service the request. Whatever resources the handlers are using to do their job need to be replicated as well."),(0,a.kt)("h2",{id:"rate-limiting"},"Rate Limiting"),(0,a.kt)("p",null,"Rate limiting constrains the number of times some kind of resource is accessed to some finite number per unit of time."),(0,a.kt)("p",null,"Rate limiting allow you to reason about the performance and stability of your system by preventing it from falling outside the boundaries."),(0,a.kt)("p",null,"In scenarios where you're chairing for access to your system, rate limits can maintain a healthy relationship with your clients. You can allow them to try the system out under heavily constrained rate limits."),(0,a.kt)("p",null,"Most rate limiting is done by utilizing an algorithm called the ",(0,a.kt)("em",{parentName:"p"},"token bucket"),"."),(0,a.kt)("p",null,"Assuming it to utilize a resource, the program has to have an ",(0,a.kt)("em",{parentName:"p"},"access token")," for the resource. Without a token, the request is denied. Now imagine these tokens are stored in a bucket, which has a finite capacity ",(0,a.kt)("inlineCode",{parentName:"p"},"d"),", waiting to be retrieved for usage. Now everytime you need to access the resource, the program reach into the bucket and remove a token, and never put it back. So when there's no token available, the program could either have to queue the request until a token becomes available, or just deny the request."),(0,a.kt)("p",null,"In order to replenish the tokens, we could define ",(0,a.kt)("inlineCode",{parentName:"p"},"r")," to be the rate at which tokens are ",(0,a.kt)("em",{parentName:"p"},"added back")," to the bucket. This becomes what we commonly think of as the rate limit."),(0,a.kt)("p",null,"The capacity ",(0,a.kt)("inlineCode",{parentName:"p"},"d")," and the replenishing rate ",(0,a.kt)("inlineCode",{parentName:"p"},"r")," help us to control both the ",(0,a.kt)("em",{parentName:"p"},"burstiness")," and overall rate limit. Burstiness simply means how many requests can be made when the bucket is full."),(0,a.kt)("details",null,(0,a.kt)("summary",null,(0,a.kt)("p",null,"There's an official implementation of a bucket token rate limit algorithm which is located in ",(0,a.kt)("inlineCode",{parentName:"p"},"golang.org/x/time/rate")," package.")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-go"},'/* helper functions */\nfunc Per(eventCount int, duration time.Duration) rate.Limit {\n  return rate.Every(duration/time.Duration(eventCount))\n}\n\n\nfunc Open() *APIConnection {\n  return &APIConnection{\n    // set the rate limit for all API connections\n    // to one event per second\n    rateLimiter: rate.NewLimiter(rate.Limit(1), 1),\n  }\n}\n\ntype APIConnection struct {\n  rateLimiter *rate.Limiter\n}\n\nfunc (a *APIConnection) ReadFile(ctx context.Context) error {\n  // wait on the rate limiter to have enough\n  // access tokens to complete our request\n  if err := a.rateLimiter.Wait(ctx); err != nil {\n    return err\n  }\n  // pass\n  return nil\n}\n\nfunc (a *APIConnection) ResolveAddress(ctx context.Context) error {\n  // wait on the rate limiter to have enough\n  // access tokens to complete our request\n  if err := a.rateLimiter.Wait(ctx); err != nil {\n    return err\n  }\n  // pass\n  return nil\n}\n\nfunc main() {\n  defer log.Printf("Done.")\n\n  log.SetOutput(os.Stdout)\n  log.SetFlags(log.Ltime | log.LUTC)\n\n  apiConnection := Open()\n\n  var wg sync.WaitGroup\n  wg.Add(20)\n\n  for i := 0; i < 10; i++ {\n    go func() {\n      defer wg.Done()\n      err := apiConnection.ReadFile(context.Background())\n      if err != nil {\n        log.Printf("cannot ReadFile: %v", err)\n        return\n      }\n      log.Printf("ReadFile")\n    }()\n  }\n\n  for i := 0; i < 10; i++ {\n    go func() {\n      defer wg.Done()\n      err := apiConnection.ResolveAddress(context.Background())\n      if err != nil {\n        log.Printf("cannot ResolveAddress: %v", err)\n        return\n      }\n      log.Printf("ResolveAddress")\n    }()\n  }\n\n  wg.Wait()\n}\n')),(0,a.kt)("p",null,"Sometimes instead of attempting to roll the semantics of limits per unit of time into a single layer, the program could keep the limiters separate and then combine them into one rate limiter that manages the interaction."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-go"},"// define a RateLimiter interface so that\n// a MultiLimiter can recursively define other\n// MultiLimiter instances\ntype RateLimiter interface {\n  Wait(context.Context) error\n  Limit() rate.Limit\n}\n\nfunc MultiLimiter(limiters ...RateLimiter) *multiLimiter {\n  byLimit := func(i, j int) bool {\n    return limiters[i].Limit() < limiters[j].Limit()\n  }\n\n  // implement an optimization and\n  // sort by the Limit() of each RateLimiter\n  sort.Slice(limiters, byLimit)\n  return &multiLimiter{limiters: limiters}\n}\n\ntype multiLimiter struct {\n  limiters []RateLimiter\n}\n\nfunc (l *multiLimiter) Wait(ctx context.Context) error {\n  for _, l := range l.limiters {\n    if err := l.Wait(ctx); err != nil {\n      return err\n    }\n  }\n  return nil\n}\n\nfunc (l *multiLimiter) Limit() rate.Limit {\n  // the multiLimiter is sorted\n  // so simply return the most restrictive limit\n  return l.limiters[0].Limit()\n}\n")),(0,a.kt)("p",null,"The ",(0,a.kt)("inlineCode",{parentName:"p"},"Wait")," method loops through all the child rate limiters and calls ",(0,a.kt)("inlineCode",{parentName:"p"},"Wait")," on each of them. These calls may or may not block, but we need to notify each rate limiter of the request so we can decrement our token bucket. By waiting for each limiter, we are guaranteed to wait for exactly the time of the longest wait. This is because if we perform smaller waits that only wait for segments of the longest wait and then hit the longest wait, the longest wait will be recalculated to only be the remaining time. This is because while the earlier waits were blocking, the latter waits were refilling their buckets; any waits after will be returned instantaneously."),(0,a.kt)("p",null,"Modify the ",(0,a.kt)("inlineCode",{parentName:"p"},"APIConnection"),":"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-go"},"func Open() *APIConnection {\n  // define the limit per second with no burstiness\n  // the limit per second will ensure the system\n  // won't overload with request\n  secondLimit := rate.NewLimiter(Per(2, time.Second), 1)\n\n  // define the limit per minute with a burstiness of 10\n  // to give the users their initial pool.\n  minuteLimit := rate.NewLimiter(Per(10, time.Minute), 10)\n\n  return &APIConnection{\n    // combine the two limits and set this as\n    // the master rate limiter for APIConnection\n    rateLimiter: MultiLimiter(secondLimit, minuteLimit),\n  }\n}\n\ntype APIConnection struct {\n  rateLimiter RateLimiter\n}\n\nfunc (a *APIConnection) ReadFile(ctx context.Context) error {\n  if err := a.rateLimiter.Wait(ctx); err != nil {\n    return err\n  }\n  // pass\n  return nil\n}\n\nfunc (a *APIConnection) ResolveAddress(ctx context.Context) error {\n  if err := a.rateLimiter.Wait(ctx); err != nil {\n    return err\n  }\n  // pass\n  return nil\n}\n")),(0,a.kt)("p",null,"Defining limits like this allows us to express our coarse-grained limits plainly while still limiting the number of requests at a finer level of detail."),(0,a.kt)("p",null,"This technique also allows us to begin thinking across dimensions other than time. When you rate limit a system, you\u2019ll likely have some kind of limit on the number of API requests, but in addition, you\u2019ll probably also have limits on other resources like disk access, network access, etc. Let\u2019s flesh out our example a bit and set up rate limits for disk and network:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-go"},"func Open() *APIConnection {\n  return &APIConnection{\n    // set up a combined rate limiter for API calls,\n    // there are limits for both requests per second, and requests per minute\n    apiLimit: MultiLimiter(\n      rate.NewLimiter(Per(2, time.Second), 2),\n      rate.NewLimiter(Per(10, time.Minute), 10),\n    ),\n    // a rate limit for disk reads.\n    // only limit this to one read per second\n    diskLimit: MultiLimiter(\n      rate.NewLimiter(rate.Limit(1), 1),\n    ),\n    // a rate limit for disk reads.\n    // limit this to three read per second\n    apiLimit: MultiLimiter(\n      rate.NewLimiter(Per(3, time.Second), 3),\n    ),\n  }\n}\n\ntype APIConnection struct {\n  networkLimit,\n  diskLimit,\n  apiLimit RateLimiter\n}\n\nfunc (a *APIConnection) ReadFile(ctx context.Context) error {\n  // when read a file, combine the limits\n  err := MultiLimiter(a.apiLimit, a.diskLimit).Wait(ctx);\n  if err != nil {\n    return err\n  }\n  // pass\n  return nil\n}\n\nfunc (a *APIConnection) ResolveAddress(ctx context.Context) error {\n  // when read a file, combine the limits\n  err := MultiLimiter(a.apiLimit, a.networkLimit).Wait(ctx);\n  if err != nil {\n    return err\n  }\n  // pass\n  return nil\n}\n"))),(0,a.kt)("h2",{id:"healing-unhealthy-goroutines"},"Healing Unhealthy Goroutines"),(0,a.kt)("p",null,"Some goroutine may fall into a bad state from which itself cannot recover without external help."),(0,a.kt)("p",null,"To heal goroutines, the program could use the heartbeat pattern to check up on the liveliness of the goroutine. The type of heartbeat will be determined by what the program is trying to monitor, but if the goroutine can become livelocked, make sure that the heartbeat contains some kind of information indicating that the goroutine is not only up, but doing useful work."),(0,a.kt)("p",null,"The logic that monitors a goroutine's health is called ",(0,a.kt)("em",{parentName:"p"},"steward"),", the monitored goroutine is called ",(0,a.kt)("em",{parentName:"p"},"ward"),". Stewards will also be responsible for restarting a ward's goroutine should it become unhealthy."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-go"},'// define the signature of a goroutine that\n// can be monitored and restarted.\ntype startGoroutineFn func(\n  done <-chan interface{},\n  pulseInterval time.Duration,\n) (heartbeat <-chan interface{})\n\n// a steward takes in a `timeout`,\n// `startGoroutine` will start the to-be-monitored goroutine,\n// the steward itself is also monitorable\nnewSteward := func(\n  timeout time.Duration,\n  startGoroutine startGoroutineFn,\n) startGoroutineFn {\n  return func(\n    done <-chan interface{},\n    pulseInterval time.Duration,\n  ) (<-chan interface{}) {\n    heartbeat := make(chan interface{})\n    go func() {\n      defer close(heartbeat)\n\n      var wardDone chan interface{}\n      var wardHeartbeat <-chan interface{}\n      // a closure that encodes a consistent way\n      // to start the goroutine parent monitoring\n      startWard := func() {\n        // create a new channel passed into the ward goroutine\n        // incase we need to signal that it should halt\n        wardDone = make(chan interface{})\n\n        // start the goroutine\n        wardHeartbeat = startGoroutine(or(wardDone, done), timeout/2)\n      }\n      startWard()\n      pulse := time.Tick(pulseInterval)\n\n  monitorLoop:\n      for {\n        timeoutSignal := time.After(timeout)\n\n        for {\n          // inner loop which ensures the steward\n          // can send  out pulses of its own\n          select {\n          case <-pulse:\n            select {\n            case heartbeat <- struct{}{}:\n            default:\n            }\n          case <-wardHeartbeat:\n            // continue the monitoring loop when\n            // receiving the ward\'s pulse\n            continue monitorLoop\n          case <-timeoutSignal:\n            // no pulse received from the ward within the timeout period\n            // request the ward halt and start a new ward goroutine\n            log.Println("steward: ward unhealyth; restarting")\n            close(wardDone)\n            startWard()\n            continue monitorLoop\n          case <-done:\n            return\n          }\n        }\n      }\n    }()\n    return heartbeat\n  }\n}\n\nlog.SetOutput(os.Stdout)\nlog.SetFlags(log.Ltime | log.LUTC)\n\ndoWork := func(done <-chan interface{}, _ time.Duration) <-chan interface{} {\n  log.Println("ward: hello, irresponsible!")\n  go func() {\n    // this goroutine isn\'t doing anything\n    // and doesn\'t sending out any pulses\n    <-done\n    log.Println("ward: halt")\n  }()\n  return nil\n}\n\n// create a function that will create a steward\n// for the goroutine `doWork` starts\ndoWorkWithSteward := newSteward(4 * time.Second, doWork)\n\ndone := make(chan interface{})\n// halt the steward and its ward after 9 seconds\ntime.AfterFunc(9 * time.Second, func() {\n  log.Println("main: halt steward and ward")\n  close(done)\n})\n\n// start the steward and range over its pulses\n// to prevent the code from halting\nfor range doWorkWithSteward(done, 4 * time.Second) {}\nlog.Println("main done")\n')),(0,a.kt)("details",null,(0,a.kt)("summary",null,(0,a.kt)("p",null,"Create a ward that will generate an integer stream based on a discrete list of values:")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-go"},'// take in the values and return any channels\n// the ward will be using to communicate back on\ndoWorkFn := func(\n  done <-chan interface{},\n  intList ...int,\n) (startGoroutineFn, <-chan interface{}) {\n  // create channel of channels as part of the bridge pattern\n  intChanStream := make(chan (<-chan interface{}))\n  intStream := bridge(done, intChanStream)\n  // create a closuer that will be started\n  // and mointored by the steward\n  doWork := func(\n    done <-chan interface{},\n    pulseInterval time.Duration,\n  ) <-chan interface{} {\n    // instantiate the channel to communicate on\n    // within this instance of the ward\'s goroutine\n    intStream := make(chan interface{})\n    heartbeat := make(chan interface{})\n    go func() {\n      defer close(intStream)\n      select {\n      case intChanStream <- intStream:\n        // let the bridge know about the new channel\n        // the programm will be communicating on\n      case <-done:\n        return\n      }\n\n      pulse := time.Tick(pulseInterval)\n\n      for {\n        valueLoop:\n        for _, intVal := range intList {\n          if intVal < 0 {\n            // simulate an unhealyth ward by\n            // logging error and return from goroutine\n            log.Printf("negative value: %v\\n", intVal)\n            return\n          }\n          for {\n            select {\n            case <-pulse:\n              select {\n              case heartbeat <- struct{}{}:\n              default:\n              }\n            case intStream <- intVal:\n              continue valueLoop\n            case <-done:\n              return\n            }\n          }\n        }\n      }\n    }()\n    return heartbeat\n  }\n  return doWork, intStream\n}\n\nlog.SetFlags(log.Ltime | log.LUTC)\nlog.SetOutput(os.Stdout)\n\ndone := make(chan interface{})\ndefer close(done)\n// create the ward function, allowing it to\n// close over the silce of integers,\n// and return a stream that it will communicate back on\ndoWork, intStream := doWorkFn(done, 1, 2, 3, -1, 4, 5)\n\n// create the steward function, monitoring doWork goroutine\n// we expect the failures fairly quickly,\n// set the period at just one millisecond\ndoWorkWithSteward := newSteward(1*time.Millisecond, doWork)\ndoWorkWithSteward(done, 1*time.Hour)\n\n// use one of the pipeline stages, take the first six values from the intStream\nfor intVal := range take(done, intStream, 6) {\n  fmt.Printf("Received: %v\\n", intVal)\n}\n'))),(0,a.kt)("p",null,"Without the benefit of a language designed around concurrency, these patterns would likely be much more cumbersome, and much less robust."))}h.isMDXComponent=!0}}]);