"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[2071],{3905:(e,t,a)=>{a.d(t,{Zo:()=>p,kt:()=>m});var n=a(67294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=n.createContext({}),c=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},p=function(e){var t=c(e.components);return n.createElement(l.Provider,{value:t},e.children)},u="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},h=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),u=c(a),h=r,m=u["".concat(l,".").concat(h)]||u[h]||d[h]||o;return a?n.createElement(m,i(i({ref:t},p),{},{components:a})):n.createElement(m,i({ref:t},p))}));function m(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=a.length,i=new Array(o);i[0]=h;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[u]="string"==typeof e?e:r,i[1]=s;for(var c=2;c<o;c++)i[c]=a[c];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}h.displayName="MDXCreateElement"},88345:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>u,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var n=a(87462),r=(a(67294),a(3905));const o={title:"Orchestration",sidebar_position:9},i=void 0,s={unversionedId:"docker/orchestration",id:"docker/orchestration",title:"Orchestration",description:"Orchestration is about running containerized apps across multiple servers -- a cluster of servers.",source:"@site/docs/docker/orchestration.md",sourceDirName:"docker",slug:"/docker/orchestration",permalink:"/docs/docker/orchestration",draft:!1,tags:[],version:"current",sidebarPosition:9,frontMatter:{title:"Orchestration",sidebar_position:9},sidebar:"tutorialSidebar",previous:{title:"Automation",permalink:"/docs/docker/automation"},next:{title:"Remote access",permalink:"/docs/docker/remote-engine"}},l={},c=[{value:"Service in Docker Swarm",id:"service-in-docker-swarm",level:2},{value:"Setting up a Docker Swarm cluster",id:"setting-up-a-docker-swarm-cluster",level:3},{value:"Running applications as Docker Swarm services",id:"running-applications-as-docker-swarm-services",level:3},{value:"Managing network traffic in the cluster",id:"managing-network-traffic-in-the-cluster",level:3},{value:"Distributed applications in Docker Swarm",id:"distributed-applications-in-docker-swarm",level:2},{value:"Docker Compose for production deployments",id:"docker-compose-for-production-deployments",level:3},{value:"Managing app configuration with config objects",id:"managing-app-configuration-with-config-objects",level:3},{value:"Managing confidential settings with secrets",id:"managing-confidential-settings-with-secrets",level:3},{value:"Storing data with volumes in the Swarm",id:"storing-data-with-volumes-in-the-swarm",level:3},{value:"Understanding how the cluster manage stacks",id:"understanding-how-the-cluster-manage-stacks",level:3},{value:"Automating release with upgrades and rollbacks",id:"automating-release-with-upgrades-and-rollbacks",level:2},{value:"The application upgrade process with Docker",id:"the-application-upgrade-process-with-docker",level:3},{value:"Configuring service rollbacks",id:"configuring-service-rollbacks",level:3},{value:"Managing downtime for the cluster",id:"managing-downtime-for-the-cluster",level:3},{value:"Understanding high availability in Swarm cluster",id:"understanding-high-availability-in-swarm-cluster",level:3},{value:"Backing up Swarm",id:"backing-up-swarm",level:3}],p={toc:c};function u(e){let{components:t,...a}=e;return(0,r.kt)("wrapper",(0,n.Z)({},p,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"Orchestration is about running containerized apps across multiple servers -- a cluster of servers."),(0,r.kt)("p",null,"An orchestrator is basically a lot of machines all grouped together to form a ",(0,r.kt)("em",{parentName:"p"},"cluster"),", teh orchestrator manages containers, distributing work among all the machines, load-balancing network traffic, and replacing any containers that become unhealthy."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Docker Engine will run on every machine in this cluster.")),(0,r.kt)("p",null,"Orchestrators do all the hard work of managing containers, and also providing features for networking, configuring applications, and storing data:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"The cluster API is a secure endpoint for administrators to manage applications.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Clusters also have a public endpoint for users to access applications on HTTP/HTTPS. This is called ",(0,r.kt)("em",{parentName:"p"},"ingress"),". External traffic can enter the cluster on any server, and it will be routed to specific containers.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"The cluster can store app configurations and secrets in its own database and deliver them to containers.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Containers can access each other anywhere on the cluster using standard networking protocols and DNS.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Clusters can also support shared storage, so containers on any server have access to the same state."))),(0,r.kt)("h2",{id:"service-in-docker-swarm"},"Service in Docker Swarm"),(0,r.kt)("h3",{id:"setting-up-a-docker-swarm-cluster"},"Setting up a Docker Swarm cluster"),(0,r.kt)("p",null,"Machines in a cluster can have different roles: they can either be a manager or a worker."),(0,r.kt)("p",null,"The difference between managers and workers is that ",(0,r.kt)("em",{parentName:"p"},"the managers run the show"),":"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"the cluster database is stored on the managers")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"you send your commands and YAML files to the API hosted on the managers")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"the scheduling and monitoring is all done by the managers"))),(0,r.kt)("p",null,"The configuration and state of the ",(0,r.kt)("em",{parentName:"p"},"swarm")," is held in a distributed ",(0,r.kt)("inlineCode",{parentName:"p"},"etcd")," database located on all managers. It\u2019s kept in memory and is extremely up-to-date. But the best thing about it is that it requires zero configuration \u2014 it\u2019s installed as part of the swarm and just takes care of itself."),(0,r.kt)("p",null,"Workers typically just run containers when the managers schedule them, and they report back on their status."),(0,r.kt)("p",null,"Something that\u2019s game changing on the clustering front is the approach to security. Swarm uses TLS to encrypt communications, authenticate nodes, and authorize roles. Automatic key rotation is also thrown in as the icing on the cake."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"# manager\ndocker swarm init\n\n# join to a swarm as worker/manager\n# docker swarm join --help\ndocker swarm join --token ${TOKEN_VALUE} HOST:PORT\n\ndocker swarm update --autolock=true\n\nservice docker restart\n\ndocker swarm unlock\n")),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"One of the big advantages of Docker Swarm over Kubernetes is the simplicity of setting up and managing the cluster. You can build a Swarm with dozens of nodes just by installing Docker on every server, running docker swarm init once, and docker swarm join for all the other nodes.")),(0,r.kt)("h3",{id:"running-applications-as-docker-swarm-services"},"Running applications as Docker Swarm services"),(0,r.kt)("p",null,"On the application orchestration front, the atomic unit of scheduling on a swarm is the service. This is a new object in the API, introduced along with swarm, and is a higher level construct that wraps some advanced features around containers. These include scaling, rolling updates, and simple rollbacks."),(0,r.kt)("p",null,"You don't run containers directly in Docker Swarm. Instead you deploy services, and the Swarm runs containers for you."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"# one service is for only one IMAGE\ndocker service create [OPTIONS] IMAGE [COMMAND] [ARG...]\n\ndocker service ls\n\ndocker service ps SERVICE_NAME/SERVICE_ID\n\ndocker service inspect SERVICE_NAME/SERVICE_ID\n\ndocker service scale SERVICE=REPLICAS\n\ndocker service rm SERVICE\n")),(0,r.kt)("p",null,"Services are defined with a lot of the same information you use to run containers. You specify the image to use, environment variables to set, ports to publish, and a name for the service that becomes its DNS name on the network."),(0,r.kt)("p",null,"The difference is that a service can have many replicas: individual containers that all use the same specification from the service and can be run on any node in the Swarm."),(0,r.kt)("p",null,"The containers that make up a service are called replicas, but they\u2019re just ordinary Docker containers."),(0,r.kt)("p",null,"You can connect to the node that\u2019s running a replica and work with it using the usual Docker container commands."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"This is one of the big differences between Docker Swarm and Docker Compose, which doesn't have a data store for application definitions. You can only manage applications with Docker Compose if you have the Compose file(s) available, because that\u2019s the source of the app definition. In Swarm mode the app definition is stored in the cluster, so you can manage apps without a local YAML file.")),(0,r.kt)("p",null,"All ",(0,r.kt)("em",{parentName:"p"},"services")," are constantly monitored by the swarm -- the swarm runs a background ",(0,r.kt)("em",{parentName:"p"},"reconciliation loop")," that constantly compares the ",(0,r.kt)("em",{parentName:"p"},"observed state")," of the service with the ",(0,r.kt)("em",{parentName:"p"},"desired state"),". If the two states match, the world is a happy place and no further action is needed. If they don't match, swarm takes actions to bring observed state into line with desired state."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"# update a service on the fly\ndocker service update [OPTIONS] SERVICE\n")),(0,r.kt)("p",null,"All container orchestrators use the approach of staged rollouts for application updates, which keeps your app online during the upgrade. Swarm implements this by replacing replicas one at a time, so if you have multiple replicas hosting your application, there are always containers running to service incoming requests."),(0,r.kt)("p",null,"Swarm also stores the previous specification of a service in its database, so if you need to manually roll back to the previous version, you can do that with a single command."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"# rollback to the previous service specification\ndocker service update --rollback SERVICE\n")),(0,r.kt)("h3",{id:"managing-network-traffic-in-the-cluster"},"Managing network traffic in the cluster"),(0,r.kt)("p",null,"Networking in Swarm mode is standard TCP/IP, as far as the applications inside containers are concerned. Components look for each other by DNS name, the DNS server in Docker returns an IP address, and the container sends network traffic to that IP address. Ultimately the traffic is received by a container and it responds."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"# create a network using overlay driver\ndocker network create --driver overlay NETWORK_NAME\n")),(0,r.kt)("p",null,"Swarm mode provides a new type of Docker network called the overlay network. It\u2019s a virtual network that spans all the nodes in the cluster, and when services are attached to the same overlay network, they can communicate with each other using the service name as the DNS name, even if they're running on different servers/nodes."),(0,r.kt)("p",null,"A DNS query to Docker for that Compose service will return the IP addresses for all the containers, and it will rely on the consumer to pick one to send the traffic to. That doesn't scale well when you have hundreds of replicas in a Swarm service, so overlay networks use a different approach and return ",(0,r.kt)("em",{parentName:"p"},"a single virtual IP address")," for the service."),(0,r.kt)("p",null,"The easiest way to see the virtual IP address (this is called VIP networking) is to connect to a terminal session in any of the container replicas. You can run some network commands to perform DNS queries on the service names and check what IP addresses are returned."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"This is VIP networking, which is supported in Linux and Windows and is a much more efficient way to load-balance network traffic. There is a single IP address from the DNS lookup, which stays constant even when the service is scaled up or down. Clients send traffic to that IP address, and the networking layer in the operating system discovers there are actually multiple destinations for the address, and it decides which one to use.")),(0,r.kt)("p",null,"Swarm uses ",(0,r.kt)("em",{parentName:"p"},"ingress networking")," to deal with multiple nodes and multiple applications: every node listening on the same port externally and Docker directing traffic internally within the cluster."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"When a request comes into ingress via certain port, Docker Swarm uses ingress networking to route the traffic. If traffic hits a node that is running containers (one or some), the requests get load-balanced among these. If the traffic hits empty because no container is running on the node, the node would transparently forward it to a node that capable.")),(0,r.kt)("h2",{id:"distributed-applications-in-docker-swarm"},"Distributed applications in Docker Swarm"),(0,r.kt)("p",null,"In a real system, you'll describe your application in a YAML file that you\u2019ll send to the swarm manager, which will then decide what actions to take to get the application running."),(0,r.kt)("p",null,"They also provide a simple way to deploy the app and manage its entire lifecycle \u2014 ",(0,r.kt)("em",{parentName:"p"},"initial deployment")," > ",(0,r.kt)("em",{parentName:"p"},"health checks")," > ",(0,r.kt)("em",{parentName:"p"},"scaling")," > ",(0,r.kt)("em",{parentName:"p"},"updates")," > ",(0,r.kt)("em",{parentName:"p"},"rollbacks")," and more!"),(0,r.kt)("h3",{id:"docker-compose-for-production-deployments"},"Docker Compose for production deployments"),(0,r.kt)("p",null,"You deploy applications in Swarm mode by creating a ",(0,r.kt)("em",{parentName:"p"},"stack"),", which is just a resource that groups together lots of other resources, such as services, networks, and volumes."),(0,r.kt)("p",null,"Stack are the first-class resource in Swarm mode."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"# deploy the stack from the Compose file:\ndocker stack deploy -c PATH_TO_COMPOSE_FILE STACK_NAME\n\n# deploy an updated Compose file for the stack with same name\ndocker stack deploy -c PATH_TO_NEW_COMPOSE_FILE STACK_NAME\n\n# details about a particular stack\ndocker stack ps STACK_NAME\n")),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Stack deployment doesn't support multiple Compose files like Docker Compose. So first you may need several Compose files. Or you can generate Compose file from multiple Compose files using ",(0,r.kt)("inlineCode",{parentName:"p"},"docker-compose -f compose-dev.yml -f compose-prod.yml > stack.yml"),".")),(0,r.kt)("p",null,"A complete example Compose file for deploying as a stack."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"version: '3.7'\n\nservices:\n  todo-web:\n    image: todo-list\n    ports:\n      - '8080:80' # default mode is _ingress mode_\n      - target: 443\n        published: 443\n        mode: host # using _host mode_\n\n    configs:\n      # Services consume config objects by specifying them in the Compose file.\n      # The formation should be consistent. JSON for json file, YAML for yml file.\n      - source: todo-list-config\n        target: /app/config/config.json\n\n    secrets:\n      # Services consume encrypted secrets objects by specifying them in the Compose file.\n      # The formation should be consistent. JSON for json file, YAML for yml file.\n      - source: todo-list-secret\n        target: /app/config/secrets.json\n    # optional configuration for stack deploying\n    #  which only make sense when running in a cluster\n    deploy:\n      replicas: 1\n      resources:\n        limits:\n          cpus: '0.50'\n          memory: 100M\n    networks:\n      - app-net\n\n  todo-db:\n    image: postgres:11.6\n    environment:\n      PGDATA: '/var/lib/postgresql/data/pgdata'\n    volumes:\n      - todo-db-data:/var/lib/postgresql/data\n    deploy:\n      replicas: 1\n      resources:\n        limits:\n          cpus: '0.50'\n          memory: 500M\n      placement:\n        # only running container in this node,\n        #     whose `labels.storage` being \"raid\"\n        constraints:\n          - node.labels.storage == raid\n          - node.role == worker\n          - node.id == <node-id>\n          - node.hostname == <host-name>\n          - node.role != manager\n          - node.labels.zone == production\n    networks:\n      - app-net\n\n  appserver:\n    image: tode-server\n    networks:\n      - app-net\n      - payment\n    deploy:\n      replicas: 2\n      update_config:\n        paralleism: 2\n        failure_action: rollback\n      placement:\n        constraints:\n          - 'node.role == worker'\n      restart_policy:\n        condition: on-failure\n        delay: 5s\n        max_attempts: 3\n        window: 120s\n    secrets:\n      - postgres_password\n\n  visualizer:\n    image: dockersamples/visualizer:stable\n    ports:\n      - '8001:8080'\n    # SIGTERM to process(PID 1), after `grace_period` time, SIGKILL\n    stop_grace_period: 1m30s\n    volumes:\n      - '/var/run/docker.sock:/var/run/docker.sock'\n    deploy:\n      update_config:\n        failure_action: rollback\n      placement:\n        constraints:\n          - 'node.role == manager'\n\n# external config objects\nconfigs:\n  todo-list-config:\n    external: true\n\nsecrets:\n  postgres_password:\n    external: true\n  todo-list-secret:\n    external: true\n\nnetworks:\n  app-net:\n  out-net:\n    external: true\n  payment:\n    driver: overlay\n    driver_opts:\n      encrypted: 'yes'\n\nvolumes:\n  todo-db-data:\n")),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Docker containers can access all the host machine\u2019s CPU and memory if you don't specify a limit. In production, you want limits to safeguard against bad code or malicious users trying to max out your system, but those limits are established only when the container starts, so if you update them you get a new container, which is a replica update in Swarm mode.")),(0,r.kt)("p",null,"Swarm stacks are a neat way of grouping applications, which you need because a cluster will typically run many apps. You can manage applications as a whole using the stack commands in the Docker CLI, listing the individual services and the service replicas or removing the app altogether."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"You can manage all the resources in a stack without needing the Compose file, because all the specifications are stored inside the cluster database. That shared database is replicated between the Swarm managers, so it\u2019s a safe place to store other resources too. It\u2019s how you\u2019ll store application configuration files in the Swarm, which you can make available to services in your Compose file.")),(0,r.kt)("h3",{id:"managing-app-configuration-with-config-objects"},"Managing app configuration with config objects"),(0,r.kt)("p",null,"Apps running in containers need to be able to load their configuration settings from the platform that is running the container."),(0,r.kt)("blockquote",null,(0,r.kt)("ul",{parentName:"blockquote"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"The Docker image is packed with default config for the dev environment.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"In test, the defaults are overridden with environment variables and local files.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"In production, the app config is loaded from config objects and secrets stored in the Swarm.")))),(0,r.kt)("p",null,"Configuration is such a critical part of deployment that all the orchestrators have a first-class resource to hold application configuration."),(0,r.kt)("p",null,"Docker Swarm supports that workflow with a specific type of resource, as config objects, that you load into the cluster from an existing configuration file."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"# create the config object from a local JSON file\ndocker config create todo-list-config ./path/to/config.json\n\ndocker config inspect --pretty todo-list-config\n")),(0,r.kt)("p",null,"Config objects are created with a name and the path to the config file contents, be it in the formation of JSON, XML, key/value pairs."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Config objects are not meant for sensitive data. The file content is not encrypted it the Swarm database, nor is it encrypted in transit as it moves frome the managers to the working nodes."),(0,r.kt)("p",{parentName:"blockquote"},"You'd better not to store database connection strings, URLs for production services and API keys.")),(0,r.kt)("h3",{id:"managing-confidential-settings-with-secrets"},"Managing confidential settings with secrets"),(0,r.kt)("p",null,"Secrets are a resource in the Swarm that the cluster manages, and they work almost exactly like config objects."),(0,r.kt)("p",null,"You create secrets from a local file, and that gets stored in the cluster database. Then you reference the secret in a service specification, and the contents of the secret get loaded into the container filesystem at runtime."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"The key difference with secrets is that you can only read them in plain text at one point in the workflow: inside the container when they are loaded from the Swarm. So you can't read the contents of the secret once it\u2019s been stored.")),(0,r.kt)("p",null,"Secrets and config objects are stored in the managers\u2019 distributed database and are available to every node."),(0,r.kt)("p",null,"One thing you do need to understand about config objects and secrets: ",(0,r.kt)("em",{parentName:"p"},"they can't be updated")," in Docker Swarm. When you create them in the cluster, the contents will ",(0,r.kt)("em",{parentName:"p"},"always be the same"),", and if you need to update the config for an application, you need to replace it:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"create a new config/secret object,")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"configure the services to use the new object,")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"deploy the stack using updated Compose file."))),(0,r.kt)("h3",{id:"storing-data-with-volumes-in-the-swarm"},"Storing data with volumes in the Swarm"),(0,r.kt)("p",null,"Volumes appear as part of the container\u2019s filesystem but they\u2019re actually stored outside of the container. Application upgrades replace the container and attach the volume to the new container, so the new container starts with all the data the previous container had."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Default volumes persist when the stack is removed.")),(0,r.kt)("p",null,"You can pin services to specific nodes, which means updates will always run on the node that has the data. That works for scenarios where you want application data to be stored outside of the container so it survives updates, but where you don't need to run multiple replicas and you don't need to allow for server failure. You apply a label to your node, and in your Compose file you restrict replicas to running on that node."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"This deployment provides guarantees for data availability, if the labeled node itself is available. If the container fails its health checks and gets replaced, the new replica will run on the same node as the previous replica and attach to the same named volume. When you update the database service specification, you get the same guarantees.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'# find the ID for one node, and update it, adding a label:\ndocker node update --label-add storage=raid $(docker node ls -q)\n\n# adding `zone` label with value "production"\ndocker node update --label-add zone=production NODE_NAME\n')),(0,r.kt)("p",null,"Applications that use disk as a data cache will be fine with local volumes, as the data can be different for each replica, but that won't work for apps that need to access shared state across the cluster."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Docker has a plugin system for volume drivers, so Swarms can be configured to provide distributed storage using a cloud storage system or a storage device in the datacenter. Configuring those volumes depends on the infrastructure you're using, but you consume them in the same way, attaching volumes to services.")),(0,r.kt)("h3",{id:"understanding-how-the-cluster-manage-stacks"},"Understanding how the cluster manage stacks"),(0,r.kt)("p",null,"Stacks in Docker Swarm are just groups of resources that the cluster manages for you:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"ingress network"),(0,r.kt)("li",{parentName:"ul"},"services, aka containers"),(0,r.kt)("li",{parentName:"ul"},"secret objects, config objects, networks"),(0,r.kt)("li",{parentName:"ul"},"volumes")),(0,r.kt)("p",null,"There are a few takeaways:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Volumes can be created and removed by the Swarm. Stacks will create a default volume if the service image specifies one, and that volume will be removed when the stack is removed. If you specify a named volume for the stack, it will be created when you deploy, but it won't be removed when you delete the stack.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Secrets and configs are created when an external file gets uploaded to the cluster. They\u2019re stored in the cluster database and delivered to containers where the service definition requires them. They are effectively write-once read-many objects and can't be updated.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Networks can be managed independently of applications, with admins explicitly creating networks for applications to use, or they can be managed by the Swarm, which will create and remove them when necessary. Every stack will be deployed with a network to attach services to, even if one is not specified in the Compose file.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Services are created or removed when stacks are deployed, and while they\u2019re running, the Swarm monitors them constantly to ensure the desired service level is being met. Replicas that fail health checks get replaced, as do replicas that get lost when nodes go offline."))),(0,r.kt)("p",null,"Stack is a logical group of components that make up an application, but it doesn't map out a dependency graph between services. You need to assume that your components will start in a random order, and capture health and dependency checks in your images so containers fail fast if the application can't run. That way the cluster can repair the damage by restarting or replacing containers, and that gets you a self-healing app"),(0,r.kt)("h2",{id:"automating-release-with-upgrades-and-rollbacks"},"Automating release with upgrades and rollbacks"),(0,r.kt)("h3",{id:"the-application-upgrade-process-with-docker"},"The application upgrade process with Docker"),(0,r.kt)("p",null,"There are at least four deployment cadences you need to consider. First there's ",(0,r.kt)("em",{parentName:"p"},"the application")," and its ",(0,r.kt)("em",{parentName:"p"},"dependencies"),", then the ",(0,r.kt)("em",{parentName:"p"},"SDK")," that compiles the app, then the ",(0,r.kt)("em",{parentName:"p"},"application platform")," it runs on, and finally the ",(0,r.kt)("em",{parentName:"p"},"operating system")," itself."),(0,r.kt)("p",null,"The build pipeline is the heart of the project. The pipeline should run every time a change to the source code is pushed. You only get confidence when releases are successful, and that's where application health checks become critical. Without them, you don't have a self-healing app, and that means you can't have safe updates and rollbacks."),(0,r.kt)("p",null,"There are some fields which configure the global service:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"mode: global"),": This setting in the ",(0,r.kt)("inlineCode",{parentName:"p"},"deploy")," section configures the deployment to run one container on every node in the Swarm. The number of replicas will equal the number of nodes, and if any nodes join, they will also run a container for the service.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"mode:host"),": This setting in the ",(0,r.kt)("inlineCode",{parentName:"p"},"ports")," section configures the service to bind directly to port on the host, and not use the ingress network. This can be a useful pattern if the web apps are lightweight enough that only one replica is need for each node, but network performance is critical, so you don't want the overhead of routing in the ingress network."))),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Service updates are always done as a staged rollout, and the default is to stop existing containers before starting new ones, freeing the host port to let the new comers take over.")),(0,r.kt)("p",null,"Docker Swarm uses cautious defaults for the rollout of service updates. It updates one replica at a time and ensures the container starts correctly before moving on to the next replica."),(0,r.kt)("p",null,"Services roll out by stopping existing containers before starting replacements, and if the update fails because new containers don't start correctly, the rollout is paused."),(0,r.kt)("p",null,"Some properties of the update configuration section change how the rollout works:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"numbers-api:\n  deploy:\n    update_config:\n      parallelism: 3\n      monitor: 60s\n      failure_action: rollback\n      order: start-first\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"parallelism")," is the number of replicas that are replaced in parallel. The default is 1, so updates roll out by one container at a time. The bigger number gives you a faster rollout and a greater chance of finding failures, because there are more of the new replicas running.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"monitor")," is thh time period the Swarm should wait to monitor new replicas before continuing with the rollout. The default is 0, and you definitely want to change that if your images have health checks, because the Swarm will monitor health checks for this amount of time. This increase confidence in the rollout.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"failure_action")," is the action to take if the rollout fails because containers don't start or fail health checks within ",(0,r.kt)("inlineCode",{parentName:"p"},"monitor")," period. The default is to pause the rollout, but you can set it to automatically roll back to the previous version.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"order"),": is the order of replacing replicas. ",(0,r.kt)("inlineCode",{parentName:"p"},"stop-first")," is the default, and it ensures there are never more replicas running than the required number. But if your app can work with extra replicas, ",(0,r.kt)("inlineCode",{parentName:"p"},"start-first")," is better because new replicas are created and checked before the old ones are removed."))),(0,r.kt)("p",null,"Parallelism can be set to around 30% of the full replica count, so your update happens fairly quickly, but you should have a monitor period long enough to run multiple health checks, so the next set of tasks only get updated if the previous update worked."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"There\u2019s one important thing to keep in mind: when you deploy changes to a stack, ",(0,r.kt)("em",{parentName:"p"},"the update configuration gets applied first"),". Then, if your deployment also includes service updates, the rollout will happen using the new update configuration."),(0,r.kt)("p",{parentName:"blockquote"},"And the update configuration settings should be included for every subsequent deployment. If you forget to include these settings, Docker Swarm will revert the service back with the default update settings.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"# view the update status of some service/container\ndocker service inspect --pretty CONTAINER_NAME\n")),(0,r.kt)("h3",{id:"configuring-service-rollbacks"},"Configuring service rollbacks"),(0,r.kt)("p",null,"There is no ",(0,r.kt)("inlineCode",{parentName:"p"},"docker stack rollback")," command; only individual services can be rolled back to their previous state."),(0,r.kt)("p",null,"Rollbacks should happen automatically when the cluster is performing a rollout and it identifies that new replicas are failing within the monitor period."),(0,r.kt)("p",null,"If any new replicas report as unhealthy during the monitor period of the rollout, that triggers the rollback action."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Using the ",(0,r.kt)("inlineCode",{parentName:"p"},"start-first")," rollout strategy helps with keeping the app running. If I used the default ",(0,r.kt)("inlineCode",{parentName:"p"},"stop-first"),", there would be a period of reduced capacity when three v3 replicas get stopped, then three v5 replicas get started and fail. In the time it takes the new replicas to flag themselves as unhealthy and for the rollback to complete, there would only be three active replicas of the API. Users wouldn't see any errors because Docker Swarm does not send any traffic to replicas that are not healthy, but the API would be running at 50% capacity.")),(0,r.kt)("p",null,"Typically, you\u2019d have the core Compose file, an environment override file, and possibly a version override file."),(0,r.kt)("p",null,"Failing health checks don't cause a rollback of the last update; they just trigger replacement replicas ",(0,r.kt)("em",{parentName:"p"},"unless the failure happens during the monitor period of the update"),"."),(0,r.kt)("details",null,(0,r.kt)("summary",null,"An example Compose file with comments."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"networks:\n  app-net:\n    name: numbers-prod\nservices:\n  numbers-api:\n    deploy:\n      replicas: 6\n      resources:\n        limits:\n          cpus: 0.5\n          memory: 75M\n      rollback_config:\n        # The goal here is to revert back as quickly as possible\n        failure_action: continue\n        # replicas of the old version will be started before the rollback worries about shutting down replicas of the new version\n        order: start-first\n        # all the failed replicas will be replaced in one go\n        parallelism: 6\n      update_config:\n        failure_action: rollback # rollback if the rollout fails\n        monitor: 60s\n        order: start-first # using `start-first` instead of `stop-first`\n        parallelism: 3\n    environment:\n      Broken: 'false'\n    healthcheck:\n      interval: 2s\n      retries: 2\n      start_period: 5s\n      test:\n        - CMD\n        - dotnet\n        - Utilities.HttpCheck.dll\n        - -u\n        - http://localhost/health\n        - -t\n        - '500'\n      timeout: 3s\n    image: diamol/ch14-numbers-api:v5\n    networks:\n      app-net: {}\n  numbers-web:\n    deploy:\n      mode: global\n      resources:\n        limits:\n          cpus: 0.75\n          memory: 150M\n    environment:\n      RngApi__Url: http://numbers-api/rng\n    healthcheck:\n      interval: 20s\n      retries: 3\n      start_period: 30s\n      timeout: 10s\n    image: diamol/ch14-numbers-web:v5\n    networks:\n      app-net: {}\n    ports:\n      - mode: host\n        published: 80\n        target: 80\nversion: '3.7'\n")),(0,r.kt)("p",null,"High-availability which health check, update config that is fast but safe, rollback config that is just fast."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yml"},"version: '3.7'\n\nservices:\n  accesslog:\n    image: access-log\n    networks:\n      - app-net\n\n  iotd:\n    image: image-of-the-day\n    ports:\n      - 8088:80\n    healthcheck:\n      test: ['CMD', 'curl', '-f', 'http://localhost/image']\n      interval: 5s\n      timeout: 20s\n      retries: 2\n      start_period: 10s\n    deploy:\n      replicas: 4\n      update_config:\n        parallelism: 2\n        order: start-first\n        monitor: 90s\n        failure_action: rollback\n      rollback_config:\n        parallelism: 4\n        order: start-first\n        failure_action: continue\n    networks:\n      - app-net\n\n  image-gallery:\n    image: image-gallery\n    ports:\n      - 80:80\n    networks:\n      - app-net\n\nnetworks:\n  app-net:\n    name: image-gallery-prod\n"))),(0,r.kt)("h3",{id:"managing-downtime-for-the-cluster"},"Managing downtime for the cluster"),(0,r.kt)("p",null,"Maintenance mode for nodes in the Swarm is called ",(0,r.kt)("em",{parentName:"p"},"drain mode"),", and you can put managers or workers into drain."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"# set a work or manager into drain mode\ndocker node update --availability drain NODE_NAME\n")),(0,r.kt)("p",null,"Drain mode means slightly different things for workers and managers. In both cases all the replicas running on the node are shut down and no more replicas will be scheduled for the node. Manager nodes are still part of the management group though, so they still synchronize the cluster database, provide access to the management API, and can be the leader."),(0,r.kt)("p",null,"What is a ",(0,r.kt)("em",{parentName:"p"},"leader manager"),"? Multiple managers are needed for high availability, but it\u2019s an active-passive model. Only one manager is actually controlling the cluster, and that\u2019s the leader. The others keep a replica of the cluster database, they can action API requests, and they can take over if the leader fails. That happens with an election process between the remaining managers, which requires a majority vote, and for that you always need an odd number of managers\u2014typically three for smaller clusters and five for large clusters."),(0,r.kt)("p",null,"If you permanently lose a manager node and find yourself with an even number of managers, you can promote a worker node to become a manager instead."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"# promote the worker to a manager\ndocker node promote WORKER_NODE\n")),(0,r.kt)("p",null,"If the node comes back, you could return one of the other managers to the worker pool by running ",(0,r.kt)("inlineCode",{parentName:"p"},"docker node demote"),"."),(0,r.kt)("p",null,"Some takeaway tips:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},"All managers go offline"),": If all your managers go offline but the worker nodes are still running, then your apps are still running. The ingress network and all the service replicas on the worker nodes work in the same way if there are no managers, but now there\u2019s nothing to monitor your services, so if a container fails it won't be replaced. You need to fix this and bring managers online to make the cluster healthy again.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},"Leader and all but one manager go offline"),": It\u2019s possible to lose control of your cluster if all but one manager node goes offline and the remaining manager is not the leader. Managers have to vote for a new leader, and if there are no other managers, a leader can't be elected. You can fix this by running swarm init on the remaining manager with the ",(0,r.kt)("inlineCode",{parentName:"p"},"force-new-cluster")," argument. That makes this node the leader but preserves all the cluster data and all the running tasks. Then you can add more managers to restore high availability.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("strong",{parentName:"p"},"Rebalancing replicas for even distribution"),": Service replicas don't automatically get redistributed when you add new nodes. If you increase the capacity of your cluster with new nodes but don't update any services, the new nodes won't run any replicas. You can re-balance replicas, so they\u2019re evenly distributed around the cluster by running ",(0,r.kt)("inlineCode",{parentName:"p"},"service update --force")," without changing any other properties."))),(0,r.kt)("h3",{id:"understanding-high-availability-in-swarm-cluster"},"Understanding high availability in Swarm cluster"),(0,r.kt)("p",null,"There are multiple layers in your app deployment where you need to consider high availability:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"health checks tell the cluster if your app is working, and the cluster will replace failed containers to keep the app online;")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"multiple worker nodes provide extra capacity for containers to be rescheduled if a node goes offline;")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"multiple managers provide redundancy for scheduling containers and monitoring workers;")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"multiple clusters in different regions to achieve datacenter redundancy;"))),(0,r.kt)("h3",{id:"backing-up-swarm"},"Backing up Swarm"),(0,r.kt)("p",null,"Backing up a swarm will backup the control plane objects required to recover the swarm in the event of a catastrophic failure of corruption, though recovering a swarm from backup is an extremely rare scenario."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Managing your swarm and applications declaratively is a great way to prevent the need to recover from a backup. You can just start the application with that same configuration file.")),(0,r.kt)("p",null,"Swarm configuration and state is stored in ",(0,r.kt)("inlineCode",{parentName:"p"},"/var/lib/docker/swarm")," on every manager node. The configuration includes; Raft log keys, overlay networks, Secrets, Configs, Services, and more. ",(0,r.kt)("strong",{parentName:"p"},"A swarm backup is a copy of all the files in this directory"),"."),(0,r.kt)("p",null,"As the contents of this directory are replicated to all managers, you can, and should, perform backups from multiple managers."),(0,r.kt)("p",null,"However, as you have to stop the Docker daemon on the node you are backing up, it\u2019s a good idea to perform the backup from non-leader managers. This is because stopping Docker on the leader will initiate a leader election."),(0,r.kt)("p",null,"You should also perform the backup at a quiet time for the business, as stopping a manager can increase the risk of the swarm losing quorum if another manager fails during the backup."),(0,r.kt)("p",null,"Steps to perform test backup, just for demonstration."),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Stop Docker on a ",(0,r.kt)("em",{parentName:"p"},"non-leader")," swarm manager."),(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"service docker stop"),". If you have any containers or service tasks running on the node, this action may stop them.")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Backup the Swarm config."),(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"tar -czvf swarm.bkp /var/lib/docker/swarm/"),". This uses the Linux ",(0,r.kt)("inlineCode",{parentName:"p"},"tar")," utility to perform the file copy that will be the backup.")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Verify the backup file exists."),(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"ls -l"),". In the real world you should store and rotate this backup in accordance with any corporate backup policies. At this point, the swarm is backed up, and you can restart Docker on the node.")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Restart Docker"),(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"service docker restart")))),(0,r.kt)("p",null,"Perform the following tasks from the swarm manager node that you wish to recover. Remember that Docker must be stopped and the contents of ",(0,r.kt)("inlineCode",{parentName:"p"},"/var/lib/docker/swarm")," must be deleted."),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Restore the Swarm configuration from backup."),(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"tar -zxvf swarm.bkp -C /"),". Restoring to the root directory is required with this command as it will include the full path to the original files as part of the extract operation. This may be different in your environment.")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Start Docker."),(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"service docker start"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Initialize a new Swarm cluster"),(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"docker swarm init --force-new-cluster"),". Remember, you are not recovering a manager and adding it back to a working cluster. This operation is to recover a failed swarm that has no surviving managers. The ",(0,r.kt)("inlineCode",{parentName:"p"},"--force-new-cluster")," flag tells Docker to create a new cluster using the configuration stored in ",(0,r.kt)("inlineCode",{parentName:"p"},"/var/lib/docker/swarm/")," that you recovered in step 1.")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Check that the network and service were recovered as part of the operation."),(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"docker network ls"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"docker secret ls"),".")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Add a new manager and worker nodes and take fresh backups."))))}u.isMDXComponent=!0}}]);