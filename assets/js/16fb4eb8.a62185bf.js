"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[7947],{3905:(e,t,n)=>{n.d(t,{Zo:()=>h,kt:()=>m});var o=n(67294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,o,a=function(e,t){if(null==e)return{};var n,o,a={},r=Object.keys(e);for(o=0;o<r.length;o++)n=r[o],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(o=0;o<r.length;o++)n=r[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=o.createContext({}),u=function(e){var t=o.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},h=function(e){var t=u(e.components);return o.createElement(l.Provider,{value:t},e.children)},c="mdxType",p={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},d=o.forwardRef((function(e,t){var n=e.components,a=e.mdxType,r=e.originalType,l=e.parentName,h=s(e,["components","mdxType","originalType","parentName"]),c=u(n),d=a,m=c["".concat(l,".").concat(d)]||c[d]||p[d]||r;return n?o.createElement(m,i(i({ref:t},h),{},{components:n})):o.createElement(m,i({ref:t},h))}));function m(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var r=n.length,i=new Array(r);i[0]=d;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[c]="string"==typeof e?e:a,i[1]=s;for(var u=2;u<r;u++)i[u]=n[u];return o.createElement.apply(null,i)}return o.createElement.apply(null,n)}d.displayName="MDXCreateElement"},23405:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>c,frontMatter:()=>r,metadata:()=>s,toc:()=>u});var o=n(87462),a=(n(67294),n(3905));const r={title:"Goroutines and the Go Runtime",sidebar_position:6},i=void 0,s={unversionedId:"concurrency-in-go/internal",id:"concurrency-in-go/internal",title:"Goroutines and the Go Runtime",description:"It's worth taking a moment to take a peek at how the runtime works, how it stitches everything together under the covers.",source:"@site/docs/concurrency-in-go/internal.md",sourceDirName:"concurrency-in-go",slug:"/concurrency-in-go/internal",permalink:"/docs/concurrency-in-go/internal",draft:!1,tags:[],version:"current",sidebarPosition:6,frontMatter:{title:"Goroutines and the Go Runtime",sidebar_position:6},sidebar:"tutorialSidebar",previous:{title:"Concurrency at Scale",permalink:"/docs/concurrency-in-go/scalability"},next:{title:"Docker",permalink:"/docs/docker/"}},l={},u=[{value:"Work Stealing",id:"work-stealing",level:2},{value:"Stealing Tasks or Continuations?",id:"stealing-tasks-or-continuations",level:2}],h={toc:u};function c(e){let{components:t,...n}=e;return(0,a.kt)("wrapper",(0,o.Z)({},h,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("p",null,"It's worth taking a moment to take a peek at how the runtime works, how it stitches everything together under the covers."),(0,a.kt)("p",null,"However, you might find these hard to understand. Challenge yourself a bit!"),(0,a.kt)("h2",{id:"work-stealing"},"Work Stealing"),(0,a.kt)("p",null,"Go runtime handles multiplexing goroutines onto OS threads for the developers. The algorithm it uses is known as a ",(0,a.kt)("em",{parentName:"p"},"work stealing")," strategy."),(0,a.kt)("p",null,"Before ",(0,a.kt)("em",{parentName:"p"},"work stealing"),", there's a naive strategy called ",(0,a.kt)("em",{parentName:"p"},"fair scheduling")," for sharing work among processors. In an effort to ensure all processors were equally utilized, we could evenly, distribute the load between all available processors."),(0,a.kt)("p",null,"However, Go models concurrency using a fork-join model. In this fork-join paradigm, tasks are likely dependent on one another, and it turns out naively splitting them among processors will likely cause one of the processors to be underutilized, as well as it can lead to poor cache locality as tasks that require the same data are scheduled on other processors."),(0,a.kt)("p",null,"We give each processor its own thread and a double-ended queue, or ",(0,a.kt)("em",{parentName:"p"},"deque"),"."),(0,a.kt)("p",null,"The work stealing algorithm follows a few basic rules. Given a thread of execution:"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},"At a fork point, add tasks to the tail of deque associated with the thread.")),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},"If the thread is idle, steal work from the head of deque associated with some other random thread.")),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},"At a join point that cannot be realized yet, pop work off the tail of the thread's own deque.")),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},"If the thread's deque is empty, either:"),(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Stall at a join")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Steal work from the head of a random thread's assocaited deque."))))),(0,a.kt)("h2",{id:"stealing-tasks-or-continuations"},"Stealing Tasks or Continuations?"),(0,a.kt)("p",null,"One thing we\u2019ve kind of glossed over is the question of what work we are enqueuing and stealing. Under a fork-join paradigm, there are two options: tasks and continuations."),(0,a.kt)("p",null,"In Go, goroutines are tasks. Everything after a goroutine is called the continuation."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-go"},"var fib func(n int) <-chan int\nfib = func(n int) <-chan int {\n  result := make(chan int)\n  /* tasks start */\n  go func() {\n    defer close(result)\n    if n <= 2 {\n      result <- 1\n      return\n    }\n    result <- <-fib(n-1) + <-fib(n-2)\n  }()\n  /* tasks end */\n  /* after goroutine, continuation */\n  return result\n}\n")),(0,a.kt)("p",null,"In the previous walkthrough of a distributed-queue work-stealing algorithm, Go runtime were enqueuing ",(0,a.kt)("em",{parentName:"p"},"tasks"),", or goroutines. Since a goroutine hosts functions that nicely encapsulate a body of work, this is a natural way to think about things."),(0,a.kt)("p",null,"However, this is not actually how Go\u2019s work-stealing algorithm works. Go\u2019s work-stealing algorithm enqueues and steals ",(0,a.kt)("strong",{parentName:"p"},(0,a.kt)("em",{parentName:"strong"},"continuations")),"."),(0,a.kt)("p",null,"To begin answering this question, let\u2019s look at the program join points."),(0,a.kt)("p",null,"Under the algorithm, when a thread of execution reaches an ",(0,a.kt)("em",{parentName:"p"},"unrealized join point"),", the thread must pause execution and go fishing for a task to steal. This is called a ",(0,a.kt)("em",{parentName:"p"},"stalling join")," because it is ",(0,a.kt)("em",{parentName:"p"},"stalling at the join while looking for work to do"),". Both task-stealing and continuation-stealing algorithms have stalling joins, but there is a significant difference in how often stalls occur."),(0,a.kt)("p",null,"Consider this: when creating a goroutine, it is very likely that your program will want the function in that goroutine to execute. It is also reasonably likely that the continuation from that goroutine will at some point want to join with that goroutine. And it\u2019s not uncommon for the continuation to attempt a join before the goroutine has finished completing."),(0,a.kt)("p",null,"Now think back to the properties of a thread pushing and popping work to/from the tail of its deque, and other threads popping work from the head. If we push the continuation onto the tail of the deque, it\u2019s least likely to get stolen by another thread that is popping things from the head of the deque, and therefore it becomes very likely that we\u2019ll be able to just pick it back up when we\u2019re finished executing our goroutine, thus avoiding a stall. This also makes the forked task look a lot like a function call: the thread jumps to executing the goroutine and then returns to the continuation after it\u2019s finished."),(0,a.kt)("p",null,"Go performs additional optimizations. Before we analyze those, let\u2019s set the stage by starting to use the Go scheduler\u2019s nomenclature as laid out in the source code."),(0,a.kt)("p",null,"From the Go scheduler's perspective, there are primarily three entities:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Goroutine (G)"),(0,a.kt)("p",{parentName:"li"},"Goroutine is the logical unit of execution that contains teh actual instructions."),(0,a.kt)("p",{parentName:"li"},"When the Go program starts, a goroutine called main goroutine is first launched, and it takes care of setting up the runtime space before starting the program.")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"OS thread or machine (M)"),(0,a.kt)("p",{parentName:"li"},"Initially, the OS threads or machines are created by and managed by the OS. Later on, the scheduler can request for more OS threads or machines to be created or destroyed. It is the actual resource upon which a goroutine will be executed. It also maintains information about the main goroutine.")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"Context or processor (P)"),(0,a.kt)("p",{parentName:"li"},"The program has a global scheduler which takes care of bringing up new OS thread, registering Goroutine, and handling system calls. However, it does not handle the actual execution of goroutines. This is done by an entity called Processor, which has its own internal scheduler and a queue called ",(0,a.kt)("inlineCode",{parentName:"p"},"runqueue")," (",(0,a.kt)("inlineCode",{parentName:"p"},"runq")," in code) consisting of goroutines that will be executed in the current context. It also handles switching between various goroutines."))),(0,a.kt)("p",null,"By the time the program is ready to start executing, the runtime would request the OS to start an ample number of Machines, GOMAXPROCS number of Processors to execute goroutines. It's important to understand that OS thread is the actual unit of execution and goroutine is teh logical unit of execution. However, they require context to actually execute goroutine against the OS thread."),(0,a.kt)("p",null,"The ",(0,a.kt)("inlineCode",{parentName:"p"},"GOMAXPROCS")," setting controls how many contexts are available for use by the runtime. The default setting is for there to be one context per logical CPU on the host machine. Unlike contexts, there may be more or less OS threads than cores to help Go's runtime manage things like garbage collection and goroutines. The runtime also contains a thread pool for threads that aren't currently being utilized."),(0,a.kt)("p",null,"Consider what would happen if any of the goroutines were blocked either by input/ output or by making a system call outside of Go\u2019s runtime. The OS thread that hosts the goroutine would also be blocked and would be unable to make progress or host any other goroutines. Logically, this is just fine, but from a performance perspective, Go could do more to keep processors on the machine as active as possible."),(0,a.kt)("p",null,"What Go does in this situation is dissociate the context from the OS thread so that the context can be handed off to another, unblocked, OS thread. This allows the context to schedule further goroutines, which allows the runtime to keep the host machine\u2019s CPUs active. The blocked goroutine remains associated with the blocked thread."),(0,a.kt)("p",null,"When the goroutine eventually becomes unblocked, the host OS thread attempts to steal back a context from one of the other OS threads so that it can continue executing the previously blocked goroutine. However, sometimes this is not always possible. In this case, the thread will place its goroutine on a global context, the thread will go to sleep, and it will be put into the runtime\u2019s thread pool for future use."),(0,a.kt)("p",null,"The global context we just mentioned doesn't fit into our prior discussions of abstract work-stealing algorithms. It\u2019s an implementation detail that is necessitated by how Go is optimizing CPU utilization. To ensure that goroutines placed into the global con\u2010 text aren't there perpetually, a few extra steps are added into the work-stealing algorithm. Periodically, a context will check the global context to see if there are any goroutines there, and when a context\u2019s queue is empty, it will first check the global context for work to steal before checking other OS threads\u2019 contexts."),(0,a.kt)("p",null,"Other than input/output and system calls, Go also allows goroutines to be preempted during any function call. This works in tandem with Go\u2019s philosophy of preferring very fine-grained concurrent tasks by ensuring the runtime can efficiently schedule work. One notable exception that the team has been trying to solve is goroutines that perform no input/output, system calls, or function calls. Currently, these kinds of goroutines are not preemptable and can cause significant issues like long GC waits, or even deadlocks. Fortunately, from an anecdotal perspective, this is a vanishingly small occurrence."))}c.isMDXComponent=!0}}]);